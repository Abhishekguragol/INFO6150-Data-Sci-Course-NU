{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Sci Engineering Methods and Tools, Lecture 4 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 26 September 2022, with material from Peter Norvig and Chris Fonnesbeck</div>\n",
    "\n",
    "# Data Wrangling with Pandas and a *practical* introduction to statistics\n",
    "\n",
    "We introduce the python analog to the `dplyr` R package, and we learn how to use it by learning a bit of statistics!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/kungfu-panda.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "`pandas` is a Python package providing fast, flexible, and expressive data structures designed to work with relational or labeled (hierarchical) data or both. It is a fundamental high-level building block for doing practical, real world, **scientific data analysis** in Python.\n",
    "\n",
    "![Protein Strucure with Sugar](ipynb.images/LysozymeRock.gif)\n",
    "\n",
    "`pandas` is well suited for:\n",
    "\n",
    "- Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet\n",
    "\n",
    "- Ordered and unordered (not necessarily fixed-frequency) *time series* (1D) data\n",
    "\n",
    "- Arbitrary *matrix* (2 and higher D) data (homogeneously typed or heterogeneous) with row and column labels\n",
    "\n",
    "- Any other form of observational / statistical data sets. The data actually need not be labeled at all to be placed into a pandas data structure\n",
    "\n",
    "Key features are:\n",
    "\n",
    "- Shape mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects\n",
    "\n",
    "- Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels, or the data can be aligned automatically\n",
    "\n",
    "- Intuitive merging and joining of data sets\n",
    "\n",
    "- Flexible reshaping and pivoting of data sets\n",
    "\n",
    "- Robust IO tools for loading data from flat files, Excel files, databases, HDF5, etc.\n",
    "\n",
    "- Build-in statistics and linear regressions, extensible with additional packages (e.g. statsmodels)\n",
    "\n",
    "After you used RStudio, you might have thought you would never use Excel functions to analyze data anymore. After today, you wil probably prefer to import your excel spreadsheet into a notebook and use pandas & friends for your data analysis.\n",
    "\n",
    "We'll also introduce **statistics** in this notebook.\n",
    "\n",
    "The most important data structures in `pandas` are 1D **Series** (vectors) and 2D **dataframes** (matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to pandas *Series*\n",
    "\n",
    "`pandas` Series are **vectors**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Series Lab\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10\n",
       "1    20\n",
       "2    30\n",
       "3    40\n",
       "4    50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = pd.Series([10,20,30,40,50])\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see how the series automatically gets its own index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 20, 30, 40, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=5, step=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify an index that we pick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.Series([10,20,30,40,50], index = ['a', 'b', 'c', 'd', 'e'])\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj['a'], obj[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query data with predicates as index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj[obj > 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyze NBA games, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = {'Celtics': 3, \"Heat\": 4}\n",
    "data = pd.Series(nba)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. I think we need more than one dimension to analyze the NBA. Once we increase the number of dimensions, we move on pandas' `DataFrame`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction to pandas *DataFrames*\n",
    "\n",
    "`pandas` dataframes are excel **spreadsheets**, also known mathematically as **matrices**!\n",
    "\n",
    "Let's import data as a dictionary structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = {\"east\": ['Celtics', \"Cavs\", \"76ers\"], \"west\": [\"Warriors\", \"Lakers\", \"Chicago\"]}\n",
    "nbadf = pd.DataFrame(nba)\n",
    "nbadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But.. does this work?\n",
    "nba = {\"east\": ['Celtics', \"Cavs\"], \"west\": [\"Warriors\", \"Lakers\", \"Chicago\"]}\n",
    "nbadf = pd.DataFrame(nba)\n",
    "nbadf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! What to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..so do this instead, and pandas adds 'None' or 'NaN' where data's missing\n",
    "nbadf = pd.DataFrame.from_dict(nba, orient='index')\n",
    "nbadf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the `east` row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf.loc['east']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf.loc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add dictionaries as the data itself, this gives pandas the opportunity to add specified indexes as rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba2 = {\"east\": {'MA': 'Celtics', 'IN': \"Cavs\"}, \"west\": {\"CA\": \"Warriors\", \"CAS\": \"Lakers\", \"IL\": \"Chicago\"}}\n",
    "nbadf = pd.DataFrame(nba2)\n",
    "nbadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf['east']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbadf.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reindex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 = nbadf.reindex(['MA', 'IN', 'IL', 'CA', 'CAS'])\n",
    "obj2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we work with data independently of indexes? Yes, using **numpy matrices**!\n",
    "\n",
    "Let's use python's `arange` to create a range of numbers, and `reshape` to shape the dimensions of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nba3 = np.arange(0,9).reshape(3,3)\n",
    "nba3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's attach the data to rows and columns to give it a context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf3 = pd.DataFrame(nba3, index=['MA', 'NY', 'TX'], columns=['Celtics', 'Knicks', 'Rockets'])\n",
    "nbadf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks.. fake. Let's use random number generator in matrix to make the data look more real!\n",
    "nbadf3 = pd.DataFrame(np.random.randn(3,3), index=['MA', 'NY', 'TX'], columns=['Celtics', 'Knicks', 'Rockets'])\n",
    "nbadf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf3['Celtics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf3.loc[:, 'Celtics'] # this is how you access columns by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf3.loc['MA', :] # this is how you access rows by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf3.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we apply math formulas to the data like we do it in excel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return x.max() - x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula as lambda\n",
    "f = lambda x: x.max() - x.min() \n",
    "nbadf3.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf3.apply(f, axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Dataframes\n",
    "Building higher dimensiona; pandas Dataframes.\n",
    "\n",
    "First, we *hack* it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 'start', array([53, 26, 22, 78, 73, 25])),\n",
       " ('one', 'end', array([53, 26, 22, 78, 73, 25])),\n",
       " ('two', 'start', array([53, 26, 22, 78, 73, 25])),\n",
       " ('two', 'end', array([53, 26, 22, 78, 73, 25])),\n",
       " ('three', 'start', array([53, 26, 22, 78, 73, 25])),\n",
       " ('three', 'end', array([53, 26, 22, 78, 73, 25]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array(['one', 'one', 'two', 'two', 'three', 'three'])\n",
    "B = np.array(['start', 'end']*3)\n",
    "C = [np.random.randint(10, 99, 6)]*6\n",
    "fake3D = list(zip(A, B, C))\n",
    "fake3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one</td>\n",
       "      <td>start</td>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>end</td>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>two</td>\n",
       "      <td>start</td>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>two</td>\n",
       "      <td>end</td>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>three</td>\n",
       "      <td>start</td>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>three</td>\n",
       "      <td>end</td>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       A      B                         C\n",
       "0    one  start  [53, 26, 22, 78, 73, 25]\n",
       "1    one    end  [53, 26, 22, 78, 73, 25]\n",
       "2    two  start  [53, 26, 22, 78, 73, 25]\n",
       "3    two    end  [53, 26, 22, 78, 73, 25]\n",
       "4  three  start  [53, 26, 22, 78, 73, 25]\n",
       "5  three    end  [53, 26, 22, 78, 73, 25]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(fake3D, columns=['A', 'B', 'C'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">one</th>\n",
       "      <th>start</th>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">two</th>\n",
       "      <th>start</th>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">three</th>\n",
       "      <th>start</th>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>[53, 26, 22, 78, 73, 25]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    C\n",
       "A     B                              \n",
       "one   start  [53, 26, 22, 78, 73, 25]\n",
       "      end    [53, 26, 22, 78, 73, 25]\n",
       "two   start  [53, 26, 22, 78, 73, 25]\n",
       "      end    [53, 26, 22, 78, 73, 25]\n",
       "three start  [53, 26, 22, 78, 73, 25]\n",
       "      end    [53, 26, 22, 78, 73, 25]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index(['A', 'B'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the above is a 3D dataframe, with the 3 \"*slices*\" one below the other, just the way numpy prints 3D tensors.\n",
    "\n",
    "We can even put the slices as columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array(C)\n",
    "df3 = pd.DataFrame(data=C.T, columns=pd.MultiIndex.from_tuples(zip(A,B)))\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, there is also a \"*proper*\" way of doing this, using the `xarray` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "xr.set_options(display_style=\"html\")\n",
    "\n",
    "times = pd.date_range(\"2000-01-01\", \"2001-12-31\", name=\"time\")\n",
    "annual_cycle = np.sin(2 * np.pi * (times.dayofyear.values / 365.25 - 0.28))\n",
    "\n",
    "base = 10 + 15 * annual_cycle.reshape(-1, 1)\n",
    "tmin_values = base + 3 * np.random.randn(annual_cycle.size, 3)\n",
    "tmax_values = base + 10 + 3 * np.random.randn(annual_cycle.size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annual_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "myplot = plt.plot(times, annual_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = plt.plot(times, tmin_values[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = plt.plot(times, tmin_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = plt.plot(times, tmax_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset(\n",
    "    {\n",
    "        \"tmin\": ((\"time\", \"location\"), tmin_values),\n",
    "        \"tmax\": ((\"time\", \"location\"), tmax_values),\n",
    "    },\n",
    "    {\"time\": times, \"location\": [\"MA\", \"VA\", \"NH\"]},\n",
    ")\n",
    "\n",
    "ds.to_array().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a dataset that has two **columns** (`tmin`, `tmax`), 731 rows, and 3 slices (`MA`, `VA`, `NH`) off the plane of the screen of your laptop.\n",
    "\n",
    "If we turn it to a pandas Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Finance with pandas\n",
    "\n",
    "Finance is cool. You can do all your financial strategizing from within a notebook with the help of pandas! I cannot *not show* you how to do this! Finance is the perfect application for `pandas` (time) **Series**.\n",
    "\n",
    "Let's install a package to read financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better do this in an anaconda terminal on windows or bash shell on mac\n",
    "!conda install pandas_datareader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that does not work, try `pip install`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better do this in an anaconda terminal on windows or bash shell on mac\n",
    "!pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas-datareader --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's *use* the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas_datareader import data, wb\n",
    "import pandas as pd\n",
    "\n",
    "# the line below is the fix for is_list_like lub\n",
    "#pd.core.common.is_list_like = pd.api.types.is_list_like\n",
    "\n",
    "import pandas_datareader as web\n",
    "import datetime\n",
    "start = datetime.datetime(2016, 1, 1)\n",
    "end = datetime.datetime(2019, 1, 30)\n",
    "aapl = web.DataReader('AAPL', 'yahoo', start, end)\n",
    "aapl.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "high = aapl['High']\n",
    "high.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = aapl['Close']\n",
    "close.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the n-th (n=1 is the default) discrete difference along a given axis to find out about gains/losses, using numpy's [diff](https://docs.scipy.org/doc/numpy/reference/generated/numpy.diff.html) API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "returns = np.diff(close)\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take the logarithm to squash big growths. In this case, does not make a big difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "returns = np.diff(np.log(close))\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high.corr(close)\n",
    "# a value of 0.3 essentially means little correlation.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, I can analyze the stock market with pandas, numpy, and matplotlib!\n",
    "\n",
    "Yes, for free! Or you can pay lots of money to Fidelity Investments, who will turn to their programmers to give you the same tools you can use in a python notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Exploratory Data Analysis (EDA)\n",
    "\n",
    "A time series is simply a series of data points ordered in time. In a time series, time is often the independent variable and the goal is usually to make a **forecast** for the future.\n",
    "\n",
    "However, there are other aspects that come into play when dealing with time series. Namely:\n",
    "- Is it **stationary**? Stationarity is a *critical* characteristic of time series. A time series is said to be\n",
    "stationary if its statistical properties do not change over time. In other words, it has\n",
    "constant mean and variance, and covariance is independent of time. We'll study what these concepts represent when we get into statistics. For now just think of them as point estimates of a distribution of numbers. Most often, stock prices are ***not a stationary process***, since we might see a growing trend, or\n",
    "its volatility might increase over time (meaning that variance is changing). Ideally, we want to have a stationary time series for modelling. Of course, not all of them are stationary, but we can often make different transformations to make them stationary. [Dickey-Fuller](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test) is the statistical test\n",
    "that we run to determine if a time series is stationary or not. If you Coop for Wall Street or get a job as quant, you'll be running this test *all the time*.\n",
    "\n",
    "\n",
    "- Is the target variable **autocorrelated**? Autocorrelation is the similarity between observations as a function of the\n",
    "time lag between them\n",
    "\n",
    "\n",
    "- Is there a **seasonality**? Seasonality refers to periodic fluctuations. For example, electricity consumption is high\n",
    "during the day and low during night, or online sales increase during Christmas before slowing down again. seasonality can also be derived from an autocorrelation plot if it has a\n",
    "sinusoidal shape. Simply look at the period, and it gives the length of the season\n",
    "\n",
    "First, we import libraries that will be helpful throughout our analysis. \n",
    "\n",
    "Then, we import a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('data/stock_prices_sample.csv', index_col=['DATE'], parse_dates=['DATE'])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we only want end of day (EOD) information, and no `GEF` or `Intraday` tickers.\n",
    "\n",
    "So let's clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.TICKER != 'GEF']\n",
    "data = data[data.TYPE != 'Intraday']\n",
    "drop_cols = ['SPLIT_RATIO', 'EX_DIVIDEND', 'ADJ_FACTOR', 'ADJ_VOLUME', 'ADJ_CLOSE', 'ADJ_LOW', \n",
    "             'ADJ_HIGH', 'ADJ_OPEN', 'VOLUME', 'FREQUENCY', 'TYPE', 'FIGI']\n",
    "\n",
    "data.drop(drop_cols, axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the closing price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot closing price\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(data.CLOSE)\n",
    "plt.title('Closing price of New Germany Fund Inc (GF)')\n",
    "plt.ylabel('Closing price ($)')\n",
    "plt.xlabel('Trading day')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, you see that this is not a stationary process, and it is hard to tell if there is some\n",
    "kind of seasonality.\n",
    "\n",
    "### Moving average\n",
    "The **moving average** model is probably the most naive-but-useful approach to time series\n",
    "modelling. This model simply states that the next observation is the **mean** of all past\n",
    "observations -in a certain window-.\n",
    "\n",
    "Although simple, this model might be surprisingly good and it represents a good starting\n",
    "point. \n",
    "\n",
    "Otherwise, the moving average can be used to identify interesting trends in the data. We\n",
    "can define a window to apply the moving average model to smooth the time series, and\n",
    "highlight different trends.\n",
    "\n",
    "Let’s use the moving average model to smooth our time series. For that, we will use a\n",
    "helper function that will run the moving average model on a specified time window and\n",
    "it will plot the result smoothed curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "#def mean_absolute_percentage_error(y_true, y_pred):\n",
    "#    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def plot_moving_average(series, window, plot_intervals=False, scale=1.96):\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    " \n",
    "    plt.figure(figsize=(17,8))\n",
    "    plt.title('Moving average\\n window size = {}'.format(window))\n",
    "    plt.plot(rolling_mean, 'g', label='Rolling mean trend')\n",
    "\n",
    "    #Plot confidence intervals for smoothed values\n",
    "    if plot_intervals:\n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        lower_bound = rolling_mean - (mae + scale * deviation)\n",
    "        upper_bound = rolling_mean + (mae + scale * deviation)\n",
    "        plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')\n",
    "        plt.plot(lower_bound, 'r--')\n",
    "\n",
    "    plt.plot(series[window:], label='Actual values')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "\n",
    "#Smooth by the previous 5 days (by week)\n",
    "plot_moving_average(data.CLOSE, 5)\n",
    "\n",
    "#Smooth by the previous month (30 days)\n",
    "#plot_moving_average(data.CLOSE, 30)\n",
    "\n",
    "#Smooth by previous quarter (90 days)\n",
    "#plot_moving_average(data.CLOSE, 90, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can hardly see a trend, because it is too close to actual curve. Let’s see\n",
    "the result of smoothing by the previous month, and previous quarter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moving_average(data.CLOSE, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moving_average(data.CLOSE, 90, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trends are easier to spot now. Notice how the 30-day and 90-day trend show a\n",
    "downward curve at the end. This might mean that the stock is likely to go down in the\n",
    "following days.\n",
    "\n",
    "### Exponential smoothing\n",
    "**Exponential smoothing** uses a similar logic to moving average, but this time, a different\n",
    "decreasing weight is assigned to each observations. In other words, less importance is\n",
    "given to observations as we move further from the present (very old observations become less important).\n",
    "\n",
    "**Double exponential smoothing** is used when there is a trend in the time series. In that\n",
    "case, we use this technique, which is simply a recursive use of exponential smoothing\n",
    "twice.\n",
    "\n",
    "**Triple exponential smoothing** extends double exponential smoothing, by adding a seasonal smoothing\n",
    "factor. Of course, this is useful if you notice seasonality in your time series.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/exponential-smoothing.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "$\\alpha$ is a smoothing factor that takes values between 0 and 1. It determines how\n",
    "fast the weight decreases for previous observations.\n",
    "\n",
    "$\\beta$ is the trend smoothing factor, and it takes values between 0 and 1.\n",
    "\n",
    "$\\gamma$ is the seasonal smoothing factor and L is the length of the season.\n",
    "\n",
    "Let's do exponential smoothing and use 0.05 and 0.3 as values for the smoothing factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def plot_exponential_smoothing(series, alphas):\n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for alpha in alphas:\n",
    "        plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "    plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Exponential Smoothing\")\n",
    "    plt.grid(True);\n",
    "        \n",
    "plot_exponential_smoothing(data.CLOSE, [0.05, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, an alpha value of 0.05 smoothed the curve while picking up most of the\n",
    "upward and downward trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    result = [series[0]]\n",
    "    \n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        \n",
    "        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - last_level) + (1 - beta) * trend\n",
    "        result.append(level + trend)\n",
    "    return result\n",
    "\n",
    "def plot_double_exponential_smoothing(series, alphas, betas):\n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "    plt.plot(series.values, label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Double Exponential Smoothing\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_double_exponential_smoothing(data.CLOSE, alphas=[0.9, 0.02], betas=[0.9, 0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "Ok, so we know data science is all about building a model of the data from the data (and not some theory). So, let's build a **model**.\n",
    "\n",
    "We must turn our series into a **stationary process** in order to model it. Therefore, let’s apply the [Dickey-Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test) to see if it is a stationary process. The Dickey-Fuller test is a point estimate, when it is tiny (say less than 0.01), we can safely say that the time series is stationary. Otherwise, *not*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels as smt\n",
    "\n",
    "def tsplot(y, lags=None, figsize=(12, 7), syle='bmh'):\n",
    "    \n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style='bmh'):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2,2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1,0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1,1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "        #smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        #smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        smt.graphics.tsaplots.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.tsaplots.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "tsplot(data.CLOSE, lags=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the Dickey-Fuller test, the time series is non-stationary. \n",
    "\n",
    "Also, looking at the autocorrelation plot, we see that it is very high, and it seems that there is no clear seasonality.\n",
    "\n",
    ">**Defintion**: Autocorrelation represents the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Autocorrelation measures the relationship between a variable’s current value and its past values.\n",
    "\n",
    ">**Note**: *Uncorrelated* does not necessarily mean *random*. Data that has significant autocorrelation is not random. However, data that does not show significant autocorrelation can still exhibit non-randomness in other ways. Autocorrelation is just one measure of randomness. In the context of **model validation** (which is the primary type of randomness we dicuss), checking for autocorrelation is typically a *sufficient test of randomness*. However, some applications require a more rigorous determination of randomness. \n",
    "\n",
    ">**Note**: Confidence intervals are drawn as a cone. By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this code are very likely a correlation and not a statistical fluke.\n",
    "\n",
    "Therefore, to get rid of the high autocorrelation and to make the process stationary, let’s take the first difference: We simply subtract the time series from itself with a lag of one day, and we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first difference to remove to make the process stationary\n",
    "data_diff = data.CLOSE - data.CLOSE.shift(1)\n",
    "\n",
    "tsplot(data_diff[1:], lags=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our series is now stationary and we can start modelling! Does all this math make you want to work for Wall Street?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/wolf-wall-street.jpg\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait professor... where's the **model**?\n",
    "\n",
    ">Patience, young padawan.. We'll dedicate a whole lecture to financial time series *soon* because it's important. I just wanted to show how to make a timer series **stationary**.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/padawan.jpg\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Interactive learning with pandas\n",
    "\n",
    "If you think  that your python professor really sucks, then you can google for good python videos on youtube, view them in a notebook, and do the computations while the instructor is talking!\n",
    "\n",
    "Now that's what I call **accelerated learning**! Here are some cool ones on **Python**, mixing *motivation* with *deep content*.\n",
    "\n",
    "By the way, that is also how scientific papers should be ***written***, with code up on github, active code within the notebook in cells that demonstrate your basic proofs, and detailed experiment setup up on youtube videos, referenced in the paper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('T5pRlIbr6gg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('uYjRzbP5aZs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('7lmCu8wz8ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here for **pandas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('B42n3Pc-N2A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('vmEHCJofslg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's one if you like, *like me*, to sing karaoke in chinese (也 你 爱  韩红, or you're bored by prof. Dino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('npQSre0yLSI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I add all songs I likee in a python notebook, keep the music and notes underneath (even download python DAW programs and run them in my notebook), and I have myself a personal KTV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data aggregation & pivoting with pandas\n",
    "\n",
    "Here we learn how to **project** and **pivot** tabular data with pandas.\n",
    "\n",
    "Let's load some gaming data and do *very basic* statistical analysis. This should remind you of similar aggregation operations with did in our **R** lab.\n",
    "\n",
    "This is a famous dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/pokemon_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reindex by sorting on `Type 1` and `HP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sort_values(['Type 1', 'HP'])\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column that is the **sum** of columns `HP` through `Speed`. This should remind you of R's **mutate** command!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total'] = df.iloc[:, 4:10].sum(axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rearrange columns to move our `Total` column (the *last* one) closer to the left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns)\n",
    "df = df[cols[0:4] + [cols[-1]]+cols[4:12]]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, better than excel!\n",
    "\n",
    "In fact, let's save our data as a spreadsheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('data/pokemon00.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter our data for `Type 1` == Grass, `Type 2` == Poison, and `HP` > 70:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]\n",
    "df3.reset_index(drop=True, inplace=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do aggregate statistics with `pandas`' GROUP_BY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.groupby(['Type 1', 'Type 2']).count()\n",
    "df4.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see, for example, that there are 12 rows of `Type 1` == Bug, `Type 2` == Poison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.groupby(['Type 1', 'Type 2']).sum()\n",
    "df4.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.groupby(['Type 1', 'Type 2']).mean()\n",
    "df4.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce the number of columns to just `HP`, `Attack`, and `Defense`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.head(20)[cols[4:7]]\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot (Wow! Plotting is built into `pandas`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a **lambda function** to compute a formula for each row that takes into account column-level statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = lambda x: (x - x.mean()) / x.std()\n",
    "df5.apply(zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we are *on par*, data manipulation wise, with what we saw on R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Statistics with pandas & friends: The Histogram\n",
    "\n",
    "`statsmodels` is a cool statistics library that also includes sample datasets for us to play with.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/sunspots.jpg\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "data_loader = sm.datasets.sunspots.load_pandas()\n",
    "df = data_loader.data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's sunspots data, from the 18th century to.. 2008! in fact, [Galileo](https://en.wikipedia.org/wiki/Galileo_Galilei) first started documenting sunspots in the 1600s, using his newly invented telescope, and reliable sunspot observations begin in about 1700.\n",
    "\n",
    "If you look closer at the dataset, you will find fractional years. When did fractional years start appearing? Take the modulo of each value with 1 to get the fractional part (lambda x: x % 1), using the `.apply()` API on the `df['SUNACTIVITY']` column. Then take the fractional part and mltiply by 1.2 to get months!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractional_nums = df['SUNACTIVITY'].apply(lambda x: x % 1) #Take the modulo of each value with 1 to get the fractional part\n",
    "fractional_nums[fractional_nums > 0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a sense of the [distribution](https://en.wikipedia.org/wiki/Probability_distribution) of the values.\n",
    "\n",
    ">**DEFINITION**: A **probability distribution** is a mathematical function that provides the probabilities of occurrence of different possible outcomes in an experiment. It is a description of a random phenomenon in terms of the probabilities of events. Random phenomena may not be as random as you think, as we will see this semester.\n",
    "\n",
    "The gaussian is, of course, one *example* of a probability distribution if the total area under the gaussian is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SUNACTIVITY'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='YEAR', y='SUNACTIVITY', xlim=(1700,2008))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the same example we saw in our multi-D pandas Dataframe example. There's a **seasonality**! In other words, a cycle of sorts (increase --> decrease --> increase --> decrease --> etc.). In fact, probably two cycles, just like our pandas Dataframe example. What is its [period](https://en.wikipedia.org/wiki/Periodic_function) (or [frequency](https://en.wikipedia.org/wiki/Frequency))?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(df['SUNACTIVITY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation clearly shows that there are two periods in the time series: A short one, and a longer one.\n",
    "\n",
    "Let's find these periods using some... math magic. This is just to show off the power of math algorithms, which are all contained in one neat package called `SciPy`, and which we leverage with the help of a few otehr tools (`numpy`, `statsmodels`, `matplotlib`, ..).\n",
    "\n",
    "We'll talk more at length about SciPy, the most beautiful of python libraries, in future notebooks.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/will-smith-aladin.jpg\" width=600 />\n",
    "Will Smith, a.k.a Alladin, masquerading as SciPy\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(df.shape)\n",
    "N2 = df.shape[0] / 2\n",
    "freqs = np.linspace(0, 0.5, num=N2, endpoint=False)[1:] #Nyquist range\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [periodogram](https://en.wikipedia.org/wiki/Periodogram#:~:text=In%20signal%20processing%2C%20a%20periodogram,methods%20(see%20spectral%20estimation).) is an estimate of the frequency components composing a signal. \n",
    "\n",
    "We use a trigonometric scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "periodogram = sp.signal.lombscargle(df['YEAR'], df['SUNACTIVITY'], freqs * 2 * np.pi)\n",
    "plt.plot(freqs, periodogram, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_index_at_max_power = np.argmax(periodogram)\n",
    "print('Frequency and corresponding time in years at max power: %.2f, %.1f' % (freqs[freq_index_at_max_power], 1 / freqs[freq_index_at_max_power]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have two cycles. The major cycle is about 11 years, and the minor cycle at a just over a month (10% of a year).\n",
    "\n",
    "**Autocorrelation** is really cool. It allows us to find *repeating* **patterns** in data. In fact, it's an underlying paradigm in Machine Learning: \n",
    "\n",
    ">**Paradigm of Machine Learning**: Machine learning is function *interpolation* of point clouds. A *function* is a dimensionality reduction of *data*. ***Intelligence*** is how many functions you can build in your lifetime to interpolate your life's experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another dataset\n",
    "Can we find any correlations (not autocorrelation, but correlation between two columns of data) in our gaming data above (not neceassarily *auto*, but of one column against another)? Let's pick the attack and defense columns, and *plot*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/pokemon_data.csv')\n",
    "attack = df['Attack']\n",
    "defense = df['Defense']\n",
    "plt.plot(attack, defense, 'g+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about for really good players?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g = df.loc[(df['Speed'] > 60) & (df['HP'] > 70)]\n",
    "df_g.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_g = df_g['Attack']\n",
    "defense_g = df_g['Defense']\n",
    "plt.plot(attack, defense, 'r.', label='all')\n",
    "plt.plot(attack_g, defense_g, 'b.', label='good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Outliers](https://en.wikipedia.org/wiki/Outlier) can really ***skew*** our data. \n",
    "\n",
    "In statistics, an outlier is a data point that ***differs significantly from other observations***. An outlier may be due to variability in the measurement or it may indicate experimental error.\n",
    "\n",
    "We need a way to find out if there are outliers in our data, remove them, and then look again at our correlations!\n",
    "\n",
    "Outliers are often taken to be any data points that are ***two standard deviations*** removed from the **mean**.\n",
    "\n",
    "Let's look at outliers for different columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of original df\n",
    "newdf = df.copy()\n",
    "\n",
    "newdf['x-Mean'] = abs(newdf['Attack'] - newdf['Attack'].mean())\n",
    "newdf['1.96*std'] = 1.96*newdf['Attack'].std()  \n",
    "newdf['Outlier'] = abs(newdf['Attack'] - newdf['Attack'].mean()) > 1.96*newdf['Attack'].std()\n",
    "outliers_df = newdf.loc[(newdf['Outlier'] == True)]\n",
    "outliers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_o = outliers_df['Attack']\n",
    "defense_o = outliers_df['Defense']\n",
    "plt.plot(attack_o, defense_o, 'r+', label='outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonoutliers_df = newdf.loc[(newdf['Outlier'] == False)]\n",
    "nonoutliers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_no = nonoutliers_df['Attack']\n",
    "defense_no = nonoutliers_df['Defense']\n",
    "plt.plot(attack_no, defense_no, 'b+', label='non outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a quadratic interpolation with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial coefficients.\n",
    "fit = np.polyfit(attack_no, defense_no, 2)\n",
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = np.poly1d(fit)\n",
    "\n",
    "new_x = attack_no.values\n",
    "new_y = poly(new_x)\n",
    "\n",
    "x = attack_no.values\n",
    "y = defense_no.values\n",
    "plt.plot(x, y, \"o\", new_x, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a very *weak* quadratic, almost a ***line*** if you ask me! That seems to indicate the our linear interpolation coefficient would be pretty high in magnitude!\n",
    "\n",
    "The Pearson correlation coefficient (named for Karl Pearson) can be used to summarize the strength of the linear relationship between two data samples.\n",
    "\n",
    "The Pearson’s correlation coefficient is calculated as the covariance of the two variables divided by the product of the standard deviation of each data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr, _ = pearsonr(x, y)\n",
    "print('Pearsons correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_no.corr(defense_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of 0.5 is actually between non-correlated and strongly correlated. We could call these two columns medium correlated, once we've removed their outliers.\n",
    "\n",
    "So, if our *average* pokemon player is good at defense, then he or she is also good at offense.\n",
    "\n",
    "This, dear class, is ***statistics***!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But **Average** and **Standard Deviation** are really only valid for [gaussian distributions](https://en.wikipedia.org/wiki/Normal_distribution) (also called *normal* distributions), as we will see later on in class.\n",
    "\n",
    "It could generalize to single-humped distributions, but definitely not to two-humped ones. What's the standard deviation of a camel (two humps)?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/camel-dromadary.png\" width=600 />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "[Histograms](https://en.wikipedia.org/wiki/Histogram) are ***essential*** to data science, because understanding *how* your data is distributed is **key** to being able to fit it to a **model**.\n",
    "\n",
    "[Here](https://www.mathsisfun.com/data/quincunx.html) is how histograms are built.\n",
    "\n",
    ">**DEFINITION**: A **model** for your data is a function that ***autoencodes your data*** (possibly as an [approximation](https://en.wikipedia.org/wiki/Approximation_theory)). In other words, it is able to learn all your data ***and*** be able to output your data by remembering a much smaller amount of data. That way, you ***keep your model and you can throw away the data***.\n",
    "\n",
    "Your brain is, in fact, a ***model builder***. It learns all your past interactions between you and your parents, brother, boyfriend, best friend, etc., and enables you to sidestep all future conflict (well, for some of us, our brain does *not* work that well ;-). How does it do that? Given the input of a potentially conflict-causing conditions, it is able to ***predict*** the future conflict, and thus devise strategies to avoid the conflict. *That* is why humans have conquered the planet. They are ***best*** at avoiding conflict.\n",
    "\n",
    "What's your most basic example of a ***model***? We talked about this... Generators!\n",
    "\n",
    "So, let's plot histograms. `seaborn` is the \\#2 visualization library, after `matplotlib`, but it has a great histogram API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of original df\n",
    "newdf3 = df.copy()\n",
    "\n",
    "Attack = newdf3.groupby('Attack')\n",
    "\n",
    "newdf3['Lower'] = Attack['Attack'].transform( lambda x: x.quantile(q=.25) - (1.5*(x.quantile(q=.75)-x.quantile(q=.25))) )\n",
    "newdf3['Upper'] = Attack['Attack'].transform( lambda x: x.quantile(q=.75) + (1.5*(x.quantile(q=.75)-x.quantile(q=.25))) )\n",
    "newdf3['Outlier'] = (newdf3['Attack'] < newdf3['Lower']) | (newdf3['Attack'] > newdf3['Upper']) \n",
    "newdf3.loc[(newdf3['Outlier'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(10,3),sharey=True,sharex=True)\n",
    "sns.distplot(newdf3[\"Attack\"], ax=ax1,\n",
    "             bins=range(newdf3[\"Attack\"].min(), newdf3[\"Attack\"].max()),\n",
    "             kde=False,\n",
    "             color=\"b\")\n",
    "ax1.set_title(\"Attack\")\n",
    "sns.distplot(newdf3[\"Defense\"], ax=ax2,\n",
    "             bins=range(newdf3[\"Defense\"].min(), newdf3[\"Defense\"].max()),\n",
    "             kde=False,\n",
    "             color=\"r\")\n",
    "ax2.set_title(\"Defense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even merge the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(nrows=1,figsize=(10,6),sharex=True)\n",
    "\n",
    "sns.distplot(newdf3[\"Attack\"], ax=ax1,\n",
    "             bins=range(newdf3[\"Attack\"].min(), newdf3[\"Attack\"].max()),\n",
    "             kde=False,\n",
    "             color=\"b\",\n",
    "             label=\"Attack\")\n",
    "sns.distplot(newdf3[\"Defense\"], ax=ax1,\n",
    "             bins=range(newdf3[\"Defense\"].min(), newdf3[\"Defense\"].max()),\n",
    "             kde=False,\n",
    "             color=\"r\",\n",
    "             label=\"Defense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Exploratory data analysis\n",
    "\n",
    "Looking at data is fun. It's like playing a game.\n",
    "\n",
    "Before you solve a problem, you look at its data, from all perspectives. That's called Exploratory Data Analysis (EDA).\n",
    "\n",
    "`Clustergrammer` is a *cool* web-based tool for looking at different perspectives and visualizing high-dimensional data (e.g. a matrix) as an interactive and shareable hierarchically clustered heatmap. \n",
    "\n",
    "Developed at the Icahn School of Medicine In New York City. Clustergrammer's front end (Clustergrammer-JS) is built using [D3.js](https://d3js.org/) and its back-end (Clustergrammer-PY) is built using Python. Clustergrammer produces highly interactive visualizations that enable intuitive exploration of high-dimensional data and has several biology-specific features to facilitate the exploration of gene-level biological data.\n",
    "\n",
    "Fernandez, N. F. et al. Clustergrammer, a web-based heatmap visualization and analysis tool for high-dimensional biological data. Sci. Data 4:170151 doi: [10.1038/sdata.2017.151 (2017)](https://www.nature.com/articles/sdata2017151).\n",
    "\n",
    "Unfortunately, it stopped working with the latest versions of pandas :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clustergrammer_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py --sys-prefix clustergrammer_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import widget classes and instantiate Network instance\n",
    "from clustergrammer_widget import *\n",
    "net = Network(clustergrammer_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import widget classes and instantiate Network instance\n",
    "from clustergrammer_widget import *\n",
    "net = Network(clustergrammer_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create random data using `numpy`'s `random` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate random matrix\n",
    "num_rows = 10\n",
    "num_cols = 10\n",
    "np.random.seed(seed=100)\n",
    "mat = np.random.rand(num_rows, num_cols)\n",
    "\n",
    "# make row and col labels\n",
    "rows = range(num_rows)\n",
    "cols = range(num_cols)\n",
    "rows = [str(i) for i in rows]\n",
    "cols = [str(i) for i in cols]\n",
    "\n",
    "# make pandas dataframe \n",
    "df6 = pd.DataFrame(data=mat, columns=cols, index=rows)\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_df(df6)\n",
    "net.cluster(enrichrgram=False)\n",
    "net.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matrix file\n",
    "net.load_file('data/rc_two_cats.txt')\n",
    "\n",
    "# cluster using default parameters\n",
    "net.cluster(enrichrgram=True)\n",
    "\n",
    "# make interactive widget\n",
    "net.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Just a tiny bit of Machine Learning\n",
    "\n",
    "Machine Learning (ML) is cool. It's what AI is all about. It's ***reverse Science***: Instead of building a *theoretical model* with traditional Science, and then use that theoretical model to predict the future, we use input/output data and from that data we build a *statistical model* of the data, and then use *that* model to predict the future. In other words, we interpolate our cloud of points, draw a curve through them, and then use the curve to do predictions.\n",
    "\n",
    "It's so cool, we need a preview of this!\n",
    "\n",
    "Here we show an example of how well `Scikit-learn` and `pandas` integrate to do Machine Learning (`Scikit-learn`) from data (`pandas`). Don't worry about the algorithm yet ([random forest](https://en.wikipedia.org/wiki/Random_forest)), or Machine Learning (ML) in general. We'll get to it eventually in class. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/pretty-forest.jpeg\" width=600 />\n",
    "</center>\n",
    "\n",
    "Random forests (RF) are cool. In my opinion, the simplest and most generally successful algorithm in ML. Invented in the 1950s (as a **decision tree** algorithm) and optimized using [ensembling](https://en.wikipedia.org/wiki/Ensemble_learning) in modern libraries like `Scikit-Learn`.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/minions-amazed.jpg\" width=300 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in meterological data as a `pandas` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nyc = pd.read_csv('data/central-park-raw.csv', parse_dates=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at our data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a bit of EDA, we decide to do some data *healing* and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data transformations\n",
    "nyc.columns = [x.strip() for x in nyc.columns]\n",
    "nyc.columns = [x.replace(' ', '_') for x in nyc.columns]\n",
    "nyc.PrecipitationIn.replace(\"T\", '0.001')\n",
    "nyc.PrecipitationIn = pd.to_numeric(nyc.PrecipitationIn.replace(\"T\", '0.001'))\n",
    "nyc['Events'] = nyc.Events.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` gives us easy integration with the `sklearn` library. \n",
    "\n",
    "So, let's build a **statistical model**!\n",
    "\n",
    "Let's see if we can use it to predict [dew point](https://en.wikipedia.org/wiki/Dew_point) (y) from the other columns (X). More specifically:\n",
    "\n",
    ">**Machine Learning** is machine-assisted statistics: Can we use measurements of the dew point and other columns in order to predict the dew point tomorrow based on the other columns?\n",
    "\n",
    "The dew point is the temperature to which air must be cooled to become saturated with water vapor. It is closely related to temperature and humidity, so knowing the latter, I expect to be able to predict the dew point, even if it is the value for the *next* day!\n",
    "\n",
    ">When the air temperature is high, the human body uses the evaporation of sweat to cool down, with the cooling effect directly related to how fast the perspiration evaporates. The rate at which perspiration can evaporate depends on how much moisture is in the air and how much moisture the air can hold. If the air is already saturated with moisture, perspiration will not evaporate. And that makes us feel *yucky*: The more unevaporated perspiration, the greater the discomfort!\n",
    "\n",
    "> Lower dew points, less than 10 °C (50 °F), correlate with lower ambient temperatures and cause the body to require less cooling. A lower dew point allows for relatively effective cooling. People inhabiting tropical and subtropical climates acclimatize somewhat to higher dew points. A resident of Singapore might have a higher threshold for discomfort than a resident of a temperate climate like Boston.\n",
    "\n",
    "Any column we want to *predict* is a ***y*** (*dependent* variable). The coluumns that help us predict it are ***x***'s (*independent* variables). And I use a capital ***X*** to denote that there are many such columns (a vector of columns, where each column is a *distribution*: a vector of values).\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/interpolation.png\" width=500 />\n",
    "</center>\n",
    "\n",
    "We will train a Random Forest with a sample of our data in order to produce a model, like the ones in the figure above.\n",
    "\n",
    ">A model is just an [interpolation](https://en.wikipedia.org/wiki/Interpolation) of data points.\n",
    "\n",
    "Then, we'll test our model with another sample to see how it performs. \n",
    "\n",
    "Let's import the `Scikit-learn` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the attributes (columns) of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " nyc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can make mean **dew point** the *dependent variable*, and predict it using other columns as *independent variables*. It would be nice if someone could tell us what clothes to wear ***tomorrow*** based on conditions ***today***, right?\n",
    "\n",
    "Let's look at mean dew point and also shift mean dew point shifted by one row ***up***, so that we may use information from the **previous day** to predict the ***next day***'s mean humidity value (isn't that what meteorologists try to do?). Indeed, each row in our spreadsheet represents meteorological data for ***one day***.\n",
    "\n",
    "`pandas` has a very neat API to do this. This is what a **row-up-shifted** column looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Shift Humidity down to predict next day\n",
    "pd.concat([nyc.MeanDew_PointF, nyc.MeanDew_PointF.shift(-1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the up-shifted dew point column as our **label**: For each row, we want to use any column *other than the upshifted dew point*, to predict the upshifted dew point.\n",
    "\n",
    "We notice that the `Events` column is **categorical** (a set of different string values), so we need to turn it into **numbers**. Pandas has a great API for this: `get_dummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make \"Dummy\" variables from Events column\n",
    "nyc_dummy = pd.get_dummies(nyc, columns=['Events'])\n",
    "nyc_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_dummy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our *training* **independent variables** (X_train) and *test* **independent variables** (X_test), and our *training* **dependent variable** (y_train) and *test* **dependent variable** (y_test). \n",
    "\n",
    "Notice how I use an uppercase X to denote that X is multi-dimensional, while I use a lowercase y to denote that y is one-dimensional.\n",
    "\n",
    "We need to remove the timestamp from the training data, as it is not relevant to our model. Each different timestamped row is just another ***observation***. We also need to disregard the `Events_` column. Where did that come from?\n",
    "\n",
    "We also need to drop `N/A`s from all cells.\n",
    "\n",
    "All that is called **EDA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to remove NA\n",
    "# Regression - Try to predict Mean_Humidity (y) from non humidity columns (X)\n",
    "# Get training set (X_train)\n",
    "def valid(col):\n",
    "    return 'Dew' not in col and 'EST' not in col and col != 'Events_'\n",
    "nyc_dummy2 = nyc_dummy.dropna()\n",
    "\n",
    "# we remove the last observation (last row) because there is no tomorrow for the last row to predict!\n",
    "X = nyc_dummy2[[x for x in nyc_dummy2.columns if valid(x)]].iloc[0:-1]\n",
    "\n",
    "# for the dependent variable, we remove the NA, so that removes the last row, too\n",
    "y = nyc_dummy2.MeanDew_PointF.shift(-1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better *view*, let's look at the transpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split the data into a training set, and a test set using the `train_test_split` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a ML model, using `RandomForestRegressor` from the `sk-learn` package. Do not worry if you don't understand how this is done. You will soon enough! \n",
    "\n",
    "That's *one* line of code in `Scikit-learn`.\n",
    "\n",
    "Then, we fit the model to the training data (that's the ***training step***). That is where we fit a **curve** to the **datapoints**. This is where we do function approximation. We try to find the function that fits through our data.\n",
    "\n",
    "\n",
    "That's *another* line of code in `Scikit-learn`.\n",
    "\n",
    "Here are your two lines of code that do *everything*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model \n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's score our model at how good it is in predicting tomorrow's mean dew point from today's weather variables (all those that *do not* relate to dew point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 measure (indicator of accuracy 1 is perfect, 0 is horrible)\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, close to 85%! Let's plot predictions over actual values to see better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert a pandas series to a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test2 = y_test.as_matrix()\n",
    "y_test2 = y_test.values\n",
    "type(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot predictions on top of test labels to see if we have a match. We'll plot one under the other and then reverse them to see if the mismatch is big or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_pred)\n",
    "plt.plot(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_test2)\n",
    "plt.plot(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a good match! Let's tabulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.Series(y_pred), y_test.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most important features in predicting mean dew point for tomorrow? We can get that from the `feature_importances_` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(X.columns, rf_model.feature_importances_),\n",
    "        key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's Min temperature, wind direction, sea level pressure, and humidity! Exactly what [Science](https://en.wikipedia.org/wiki/Dew_point) says it should be! If it's not, we need to revise the model (or the science!).\n",
    "\n",
    "## Conclusion\n",
    "We see that statistics can be used to create a **model**, and to predict the **future** from it given a piece of correlated data. \n",
    "\n",
    "Statistics can do what Science can, absent a theory! \n",
    "\n",
    "You know what? I think it's the best tool for ridding humanity of politicians! We can just have good data scientists, like *you*, create models from social data, and use these models to predict the better strategies for the future. Then, we would need less politicians that often ruin our lives..\n",
    "\n",
    "But if you want to be a good data scientist, you need to start learning pandas..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
