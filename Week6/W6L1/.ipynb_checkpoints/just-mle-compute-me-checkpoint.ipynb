{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Sci Engineering Methods and Tools, Week 6 Lecture 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 11 October 2022</div>\n",
    "\n",
    "The **Method of Moments** (MOM) is the simplest method to find out the parameters of an analytic distribution that matches your empirical distribution. But is it the best? \n",
    "\n",
    ">**Caution**: This notebook is a bit *heavy* on math. Even though the math may appear scary in the beginning, it will be good for you to study it well because the math for artificial neural networks (ANN) is very similar.\n",
    "\n",
    "Here's the essence of what we're going to do: We saw that the idea behind MOM is to match the histogram of a dataset with the histogram of known analytic distributions, pick one, and then to derive its analytic parameters by equating analytic moments with empirical moments.\n",
    "\n",
    "So it comes down to estimating the right paramaters for our analytic model.\n",
    "\n",
    "Is there a more precise way to evaluate these parameters?\n",
    "\n",
    "Ye, there is, using optimization theory and derivatives. What we'll cover today: **Maximum Likelihood Estimation** (MLE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"divCheckbox\" style=\"display: none;\">\n",
    "\"20102020\"[::-1]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. An experiment with a biased coin\n",
    "Let's build a **model** for a fair coin toss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "probs = np.full((2), 1/2)\n",
    "face = [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(face, probs)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Coin Toss Outcome', fontsize=12)\n",
    "plt.title('Coin Toss Bernoulli Distribution', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a **model** (histogram) for a *biased* coin toss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.array([0.75, 0.25])\n",
    "face = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(face, probs)\n",
    "plt.title('Loaded coin Bernoulli Distribution', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Loaded coin Outcome', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. An experiment with a biased die\n",
    "\n",
    "- How to model throwing a die with data science? Is that a perfectly random event, or not? What is the histogram that matches the experiment? Build a **model**.\n",
    "\n",
    "- Build a **weighted die** that matches a gaussian distribution instead (in other words, build a model for it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5,6]\n",
    "counts, bins = np.histogram(x, bins= 100)\n",
    "bins = bins[:-1] + (bins[1] - bins[0])/2\n",
    "probs = counts/float(counts.sum())\n",
    "print(probs.sum()) # 1.0\n",
    "plt.bar(bins, probs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.full((6), 1/6) #Return a new array of shape (6) and type, filled with 1/6\n",
    "face = [1,2,3,4,5,6]\n",
    "plt.bar(face, probs)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Die Roll Outcome', fontsize=12)\n",
    "plt.title('Fair Die follows a Uniform Distribution', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded (unfair) die:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.array([0.25, 0.125, 0.05, 0.4, 0.125, 0.05])\n",
    "face = [1,2,3,4,5,6]\n",
    "plt.bar(face, probs)\n",
    "plt.title('Loaded die (not uniform distribution)', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Die Roll Outcome', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded die with a gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "n = np.arange(1, 7)\n",
    "mean = 3.5\n",
    "gaussian_dice = stats.norm.pdf(n, mean)\n",
    "plt.plot(n, gaussian_dice)\n",
    "\n",
    "face = [1,2,3,4,5,6]\n",
    "plt.bar(face, gaussian_dice)\n",
    "plt.title('Loaded dice (gausian distribution)', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Dice Roll Outcome', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: none;\">\n",
    "x = [0,1,2,3,4,5,6]\n",
    "counts, bins = np.histogram(x, bins= 100)\n",
    "bins = bins[:-1] + (bins[1] - bins[0])/2\n",
    "probs = counts/float(counts.sum())\n",
    "print(probs.sum()) # 1.0\n",
    "plt.bar(bins, probs)\n",
    "plt.show()\n",
    "\n",
    "probs = np.full((6), 1/6)\n",
    "face = \\[1,2,3,4,5,6]\n",
    "plt.bar(face, probs)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Dice Roll Outcome', fontsize=12)\n",
    "plt.title('Fair Dice Uniform Distribution', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "\n",
    "probs = np.array([0.25, 0.125, 0.05, 0.4, 0.125, 0.05])\n",
    "face = [1,2,3,4,5,6]\n",
    "plt.bar(face, probs)\n",
    "plt.title('Loaded dice (not uniform distribution)', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Dice Roll Outcome', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "\n",
    "n = np.arange(1, 7)\n",
    "mean = 3.5\n",
    "gaussian_dice = stats.norm.pdf(n, mean, 0.9)\n",
    "plt.plot(n, gaussian_dice)\n",
    "\n",
    "face = [1,2,3,4,5,6]\n",
    "plt.bar(face, gaussian_dice)\n",
    "plt.title('Loaded dice (gausian distribution)', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Dice Roll Outcome', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these hints, do your homework *differently* than professor, and also build a **generative model** allowing you to actually roll your gaussian-loaded die and generate **random variates**. Then, generate 100 random variates from it. Plot their histogram. How close does it match the theoretical histogram? Evaluate the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**Maximum likelihood estimation** (MLE) fitting is usually ***more work*** than the **method of moments** (MOM), but is ***preferred*** as the resulting estimator is known to have ***good theoretical properties***. MLE also uses the\n",
    "same math principles as Neural Network models, namely *minimize a function by finding where its derivative is zero*, a.k.a. [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "\n",
    "MLE is a method of estimating the *parameters* of a statistical model, given observations. MLE attempts to find the parameter values that ***maximize the likelihood function, given the observations*** (remember the **likelihood factor** in **Bayes' formula**?). \n",
    "\n",
    ">**Essence**: We attempt to find the values of the parameters (estimators) which would most likely, from a probability perspective, produce the data that we observed. \n",
    "\n",
    "We usually go from model to data. We do not have the model, but we do have the data. So it is an ***inverse problem*** (btw, Inverse problems was the topic of my PhD thesis :-).\n",
    "\n",
    "[Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss) and [Pierre-Simon Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace) where early users of maximum likelihood. [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) popularized it between 1912 and 1922, but it remained rigorously unproven until [Samuel S. Wilks](https://en.wikipedia.org/wiki/Samuel_S._Wilks) in 1938. [Wilks' theorem](https://en.wikipedia.org/wiki/Wilks%27_theorem) shows that the ***error in the logarithm of likelihood values for estimates from multiple independent samples is [asymptotically distributed](https://en.wikipedia.org/wiki/Asymptotic_distribution)***. Wilks gave his most general proof of the theorem in 1962.\n",
    "\n",
    ">**Note**: MLE is used often with **count models** (general rule of thumb for count models is that it is risky to use ML with samples smaller than 100, while samples over 500 seem adequate, so if you have only a couple hundred datapoints, ML won't work! In which case you need to fall back to MLE), **and** when we are interested in a dataset $(x,y)$ but are unable to obtain $y$ for the entire population $x$ and only able to obtain the $y$'s for a ***subset*** of $x$. \n",
    "\n",
    "A general approach in statistics is that if we are unable to obtain $y$ for the entire population $x$, it is common to assume all $y$ are **normally (gaussian) distributed** with some unknown **mean** and **variance**. The mean and variance are estimated with MLE while only knowing the $y$'s of a subset of $x$. \n",
    "\n",
    ">**Central Limit Theorem**: The [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) states that, given certain conditions, the mean of the $y_i$s of a large number of iterates of independent random variables $x_i$s will be approximately **normally distributed**, regardless of the underlying distribution. \n",
    "\n",
    "<br />\n",
    "<img src =ipynb.images/clt.png width = 600 />\n",
    "\n",
    "Let’s choose an exponential distribution with lamda equal to two.\n",
    "\n",
    "We will draw one thousand random samples of size two from this exponential distribution, take the mean of each pair of two, and plot the histogram of the results.\n",
    "\n",
    "In this case, the $n$ would be two, and as we can observe the distribution doesn’t look like a normal distribution. If we take samples of size ten ($n$ is now ten) and repeat the previous process, the distribution is a little bit more normal. And as $n$ gets larger, it’s easy to see how the distributions of the sample mean looks more like a normal distribution.\n",
    "\n",
    "<br />\n",
    "<img src =ipynb.images/clt2.png width = 800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import exponential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "  \n",
    "# Using exponential() method\n",
    "exp_variates = np.random.exponential(2.0, 100000)\n",
    "plt.hist(exp_variates, 20, density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 1, 100000), exp_variates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 1, 100), exp_variates[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean of 2 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_2 = [(x + exp_variates[i+1]) / 2. for i,x in enumerate(exp_variates[:-1])]\n",
    "plt.hist(mean_2, 20, density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 1, 100), mean_2[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean of 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_10 = [sum(exp_variates[i:i+9]) / 10. for i in range(0, 100000, 10)]\n",
    "plt.plot(np.linspace(0, 1, 100), mean_10[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean of 100 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_100 = [sum(exp_variates[i : i + 99]) / 100. for i in range(0, 100000, 100)]\n",
    "plt.plot(np.linspace(0, 1, 100), mean_100[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, agreggating data into moments is a statistically normalizing operation in that the pdf looks more ***normal***.\n",
    "\n",
    "Now, here's the secret sauce of the MLE algorithm:\n",
    "\n",
    "- MLE takes the mean and variance as **parameters**, and finds values for these parameters that make the observed results (the subset) the ***most probable given a gaussian model***. The analysis is an **iterative** one, which proceeds until a metric called the **log likelihood** *converges*.\n",
    "\n",
    "You will see it does sound very much like **variational inference** and **Markov Chain Monte Carlo** (MCMC) methods (when we'll look at these in class), but it's actually ***simpler***. From the point of view of Bayesian inference, MLE is a special case of [**maximum a posteriori estimation**](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) (MAP) that assumes a **uniform prior** distribution of the parameters. Priors are avoided by not making probability statements about the parameters, but only about their estimates, whose properties are fully defined by the observations and the statistical model.\n",
    "\n",
    "First, let's start with a practical introduction to the notion of a [derivative](https://en.wikipedia.org/wiki/Derivative).\n",
    "\n",
    ">**DEFINITION**: The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function f(x) and mathematically evaluate f'(x)\n",
    "def f(x):\n",
    "    return (x/5)**3\n",
    "\n",
    "def fp(x):\n",
    "    return (3./5.)*(x/5)**2\n",
    "\n",
    "def g(x):\n",
    "    return x**3\n",
    "\n",
    "def h(x):\n",
    "    return x/5\n",
    "\n",
    "def f_verify(x):\n",
    "    return g(h(x))\n",
    "\n",
    "def f_verify_p(x):\n",
    "    return hp(x)*gp(h(x))\n",
    "    # 1/5 * 3 x**2 = 3/5 * x**2\n",
    "\n",
    "# Interval x\n",
    "xleft = -10.\n",
    "xright = 10.\n",
    "\n",
    "# Restrict the y range for nicer plots [optional]\n",
    "ybottom = -8.\n",
    "ytop = 8.\n",
    "\n",
    "# plot the curve and derivative\n",
    "X = np.linspace(xleft, xright, 1000)\n",
    "Y = [f(x) for x in X]\n",
    "Z = [fp(x) for x in X]\n",
    "plt.plot(X,Y)\n",
    "plt.plot(X,Z)\n",
    "#slider = plot_tangent(f,fp,xleft,xright,ybottom,ytop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(4/x) + np.cos(x)# function goes here\n",
    "\n",
    "def fp(x):\n",
    "    return (-4*x**-2)*np.cos(4/x) - np.sin(x)# derivative goes here\n",
    "\n",
    "# x interval\n",
    "xleft = 0.3\n",
    "xright = 7\n",
    "\n",
    "# y interval\n",
    "ybottom = -0.5\n",
    "ytop = 2\n",
    "\n",
    "# plot the curve and derivative\n",
    "X = np.linspace(xleft, xright, 1000)\n",
    "Y = [f(x) for x in X]\n",
    "Z = [fp(x) for x in X]\n",
    "plt.plot(X,Y)\n",
    "plt.plot(X,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Example data set\n",
    "\n",
    "Say we have some data $y = y_1,y_2,\\ldots,y_n$ that is distributed according to some distribution:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$Pr(Y_i=y_i \\; | \\;\\lambda)$$\n",
    "</div>\n",
    "\n",
    "Assume the data is drawn from a Poisson distribution with parameter $\\lambda =5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = np.random.poisson(5, size=100)\n",
    "#plt.hist(y, bins=12, normed=True)\n",
    "#plt.xlabel('y'); plt.ylabel('Pr(y)')\n",
    "\n",
    "y = np.random.poisson(5, size=100)\n",
    "\n",
    "print(list(y))\n",
    "plt.figure(0)\n",
    "plt.plot(list(y))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.hist(y, bins=12, density=True)\n",
    "plt.xlabel('y'); plt.ylabel('Pr(y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 The theory\n",
    "\n",
    "The **likelihood function $l$** calculates the joint probability of observing $all$ the values of the dependent variable in our dataset, ***one after the other***, when we evaluate our model. So it's an **intersection** of events (really the intersection of *all possible events*), thus a **product of probabilities**. \n",
    "\n",
    "It's just like if i told you I throw a coin and I observe 10 successive *heads*. The probability of that event is the product of 0.5 ten times.\n",
    "\n",
    "It assumes that each observation is drawn *randomly* and *independently* from the population. If the values of the dependent variable are random and independent, then you can find the joint probability of observing all the values one after the other by multiplying the individual density functions.\n",
    "\n",
    "$$l = \\prod_{i=1}^n Pr(y_i \\;| \\;\\lambda)$$ \n",
    "\n",
    "$l$ will give us a measure of how **likely we are** to observe values $y_1,\\ldots,y_n$ given the parameter $\\lambda$. \n",
    "\n",
    ">**Definition**: **Maximum likelihood fitting** consists of **maximizing $l$** with respect to $\\lambda$ so that this outcome is **the most likely** for the best possible parameter $\\lambda$. We call this function the *likelihood function*, because it is a measure of ***how likely the observations are if the model is true***.\n",
    "\n",
    "We are essentially doing a kind of Bayesian inference: Instead of saying: ***the evidence are the observations, how do we get to the model?***, we say ***given the evidence of a model with an unknown parameter, how likely are the observations***? \n",
    "\n",
    "We come up with an equation that involves the parameter, which we want to maximize. This in turn yields the right parameter for the model. So, instead of using moments as we did with MOM, we find maxima on a function (which is also what ANNs, do by the way). It is likely how ***your*** brain builds models: What are the right parameters so that my model yields my observations *most probably*?\n",
    "\n",
    "And so we want to find those points where the derivative of the likelihood function is zero.\n",
    "\n",
    "And you know what? The likelihood function can be ***computed analytically***, in closed form, ***for all popular analytic distribution models***! Just like the moments, for all popular distribution models. That's why we catalogue useful analytic distributions, so we can pre-evaluate their properties analytically.\n",
    "\n",
    "So, let's recap: \n",
    "\n",
    "- The product $\\prod_{i=1}^n Pr(y_i \\; | \\; \\theta)$ gives us a measure of how **likely** it is to observe values $y_1,\\ldots,y_n$ given the parameters $\\lambda$. MLE consists in choosing the appropriate function $l= Pr(y|\\theta)$ to maximize for a given set of observations. This function is called the *likelihood function*, because it is a measure of how likely the observations are if the model is true.\n",
    "\n",
    "In the above model, the data were drawn from a Poisson distribution with parameter $\\lambda =5$, That is,\n",
    "\n",
    "$$L(x|\\lambda=5) = \\frac{e^{-5} 5^x}{x!}$$\n",
    "\n",
    "Instead of looking at the function above as a function of $x$, let us now look at it as a *function of* $\\lambda$. Let's calculate the likelihood that the underlying process with $\\lambda = 5$ generates any given value of $y$ (note the sexy factorial evaluation below). But let's start with $x$ = 10.\n",
    "\n",
    "For any given value of $x$, we can calculate its likelihood. We will use a python `lambda` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_like = lambda x, lam: np.exp(-lam) * (lam**x) / (np.arange(x)+1).prod()\n",
    "\n",
    "lam = 5\n",
    "value = 10\n",
    "poisson_like(value, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability above is for $x$ = 10. What is the probability for *all* $x$ datapoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [poisson_like(xi, lam) for xi in {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14}]\n",
    "probs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thus the probability of observing all our empirical datapoints one after the other, given the assumed model (with two different ways of evaluating the product of a list):\n",
    "\n",
    "(two ways of computing it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce \n",
    "reduce((lambda x, y: x * y), probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the same result for $\\lambda$ = 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 4\n",
    "probs2 = [poisson_like(xi, lam) for xi in {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14}]\n",
    "probs2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 4\n",
    "np.prod(probs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, both probabilities are ***very very low*** because we're evaluating a joint probability of ***a lot*** of events, but the probability for $\\lambda = 8$ is ***lower***! Thus, $\\lambda$ = 5 is a ***better*** value, more ***likely*** to lead our model to reproduce our entire dataset (it gives us a ***higher probability to observe our $y$'s***)!\n",
    "\n",
    ">***duh***, of course it is, the data was drawn from a random Poisson process with $\\lambda = 5$, but *we don't know that -well, we do, but we're pretending we don't* :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the likelihood function for any value of $\\lambda$, for a particular $x$ (let's pick $x$ = 5, arbitrarily):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0,15)\n",
    "_x = 5\n",
    "plt.plot(lambdas, [poisson_like(_x, l) for l in lambdas])\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('L($\\lambda$|x={0})'.format(_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like $\\lambda$ = 5 is about right for $y$ = 5.\n",
    "\n",
    "Try it out for other values of $y$. Does the optimal $\\lambda$ remain the same? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the likelihood function different than the probability distribution function (pdf)? The likelihood is a function of the parameter(s) *given the data*, whereas the pdf returns the probability of the data given a particular parameter value. \n",
    "\n",
    "Here is the pdf of the Poisson for $\\lambda=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 5\n",
    "xvals = np.arange(15)\n",
    "plt.bar(xvals, [poisson_like(x, lam) for x in xvals])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Pr(X|$\\lambda$=5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, both curves look the same (kinda), but that is a coincidence. *It's not always like this*.\n",
    "\n",
    "So we cannot locate one $\\lambda$ that is optimal ***for all $y$'s***. So we have to ***compromise***. Specifically, we want the value of $\\lambda$ which **maximizes the likelihood function**, because it yields a pdf that is the closest to the histogram of the data. \n",
    "\n",
    "In other words, our observations, which are a subset of all possible data which we can observe and record for a specific physical process, yield a histogram that we assume matches the pdf of all the data. So we model all possible data as a parametrized Poisson pdf. The Poisson pdf matches the histogram of the observed data the best when the likelihood function is maximal. And that happens when the area under the curve is maximal.\n",
    "\n",
    "So, inference is reduced to an **optimization problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 The intuition behind the theory\n",
    "\n",
    "Suppose that before a race, Max Verstappen accrues 284 points out of 1,207 total points awarded. Having this data, we’d like to make a guess at the probability that Max wins the race.\n",
    "\n",
    "The simplest guess here would be 284/1,207 = 24%, which is the best possible guess based on the data. \n",
    "\n",
    "For the sake of argument, let's say Max won 10 out of 20 races.\n",
    "\n",
    "Let's simplify that Max has a single winning probability (let’s call this θ) throughout all races across the season, regardless of the uniqueness of each one. In other words, we’re assuming each of CV's events as a [Bernoulli trial](https://en.wikipedia.org/wiki/Bernoulli_trial) with a winning probability θ.\n",
    "\n",
    "With this assumption, we can describe the probability that CV wins k times ***out of n races*** for any given number k and n (k≤n). More precisely, we assume that the number of event wins for CV follows a [binomial distribution with parameter θ](https://en.wikipedia.org/wiki/Binomial_distribution).\n",
    "The probability that CV wins k times out of n events, given the winning probability θ, is:\n",
    "\n",
    "$$P(\\text{k wins out of n events} \\; | \\; θ) = (^n_k) \\; θ^k (1 - θ)^{n-k}$$\n",
    "\n",
    "This simplification (describing the probability using just a single parameter θ regardless of real world complexity) is our statistical **model**, and θ is the **parameter** to be estimated.\n",
    "\n",
    "Since we have observed data for this season, which is 10 out of 20 wins for CV (let’s call this data as D), we can calculate P(D|θ) — the probability that this data D is observed for given θ. \n",
    "\n",
    "Let’s calculate P(D|θ) for $θ=0.1$ as an *example*:\n",
    "\n",
    "$$P(\\text{10 wins out of 20} \\; | \\; θ) = (^{20}_{10}) \\; 0.1^{10} (0.9)^{10}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "P_10_wins_out_of_20_and_theta = m.factorial(20) // m.factorial(10) // m.factorial(10) * 0.1**10 * 0.9**10\n",
    "P_10_wins_out_of_20_and_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a ***very low probability***! So, if Max’s winning probability θ is actually 0.1, this data D (10 wins in 20 races) is ***extremely unlikely to be observed***.\n",
    "\n",
    "Oh oh.\n",
    "\n",
    "Then what if θ = 0.7?\n",
    "\n",
    "$$P(\\text{10 wins out of 20} \\; | \\; θ) = (^{20}_{10}) \\; 0.7^{10} (0.3)^{10}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_10_wins_out_of_20_and_theta = m.factorial(20) // m.factorial(10) // m.factorial(10) * 0.7**10 * 0.3**10\n",
    "P_10_wins_out_of_20_and_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woooooow! That's a ***much higher probability***! \n",
    "\n",
    "So if Max’s winning probability θ is 0.7, this data D is ***much more likely to be observed*** than when θ = 0.1.\n",
    "\n",
    "Based on this comparison, we would be able to say that θ is more likely to be 0.7 than 0.1 considering the actual observed data D. \n",
    "\n",
    ">**Note**: Instead of saying *the evidence are the observations, how do we get to the model*?, we say *given the evidence of a model with an unknown parameter, how likely are the observations*?\n",
    "\n",
    "Here, we’ve been calculating the probability that D is observed for each θ, but at the same time, we can also say that we’ve been checking likelihood of each value of θ based on the observed data. Because of this, P(D|θ) is also considered as Likelihood of θ. \n",
    "\n",
    "The next question here is, what is the *exact value* of θ which maximizes the likelihood P(D|θ)? This is Maximum Likelihood Estimation!\n",
    "\n",
    "The value of θ maximizing the likelihood can be obtained by having derivative of the likelihood function with respect to θ, and setting it to zero.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/mle-math.png width = 600 />\n",
    "</center>\n",
    "\n",
    "Since likelihood goes to zero when θ= 0 or 1, the value of θ that maximizes the likelihood is $\\frac{k}{n}$.\n",
    "\n",
    "$$ θ = \\frac{k}{n}$$\n",
    "\n",
    "In other words, the estimated value of θ, CV's winning percentage per event, is 10/20 = 50% when estimated with MLE. And that's also the intuitive value you would've picked if you knew that CV wins 10 out of 20 races, right? :-)\n",
    "\n",
    "So, Bayesian statistics agrees with frequentist statistics when the result is clear-cut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Nashville rainfall\n",
    "Let's go back to our rainfall data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip = pd.read_table(\"data/nashville_precip.txt\", index_col=0, na_values='NA', delim_whitespace=True)\n",
    "precip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put MLE to use with our Nashville rainfall data, where we used a gamma distribution instead of a Poisson distribution. So we need to maximize:\n",
    "\n",
    "$$l = \\prod_{i=1}^n \\beta^{\\alpha} x^{\\alpha-1} e^{-x/\\beta}\\Gamma(\\alpha)^{-1}$$ with respect to $(\\alpha, \\beta)$\n",
    "\n",
    "In order to make the likelihood function more manageable (this is legit since `log` is monotonic with respect to its argument), the optimization is performed ***using a natural log transformation of the likelihood function***. In other words, we apply a **kernel method** transformation, which we learned about in our previous notebook. \n",
    "\n",
    "We are going to use the property that the *log of a product is the sum of the logs*:\n",
    "\n",
    "$$ log(ab) = log(a) + log(b)$$\n",
    "\n",
    "We want to maximize:\n",
    "\n",
    "$$\\begin{align}log(l)(\\alpha,\\beta) &= \\sum_{i=1}^n \\log[\\beta^{\\alpha} x^{\\alpha-1} e^{-x/\\beta}\\Gamma(\\alpha)^{-1}] \\cr \n",
    "&= n[(\\alpha-1)\\overline{\\log(x)} - \\bar{x}\\beta + \\alpha\\log(\\beta) - \\log\\Gamma(\\alpha)]\\end{align}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\bar{x} = \\frac{\\sum_i{x_i}}{n}$$\n",
    "\n",
    "(*so much easier to work in the log scale because the log of a product is the sum of the logs!*)\n",
    "\n",
    "where $n = 2012 − 1871 = 141$ and the bar indicates an average over all *i*. We want to choose $\\alpha$ and $\\beta$ to maximize $l(\\alpha,\\beta)$.\n",
    "\n",
    "Notice $l$ is infinite if any $x$ is zero. We do not have any zeros, but we do have an NA value for one of the October data, which we dealt with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Solving the equation\n",
    "\n",
    "To find the maximum of any function, we typically take the *derivative* with respect to the variable to be maximized, set it to zero and solve for that variable. \n",
    "\n",
    "$$\\frac{\\partial log(l)(\\alpha,\\beta)}{\\partial \\beta} = n\\left(\\frac{\\alpha}{\\beta} - \\bar{x}\\right) = 0$$\n",
    "\n",
    "Which can be solved as $\\beta = \\alpha\\; / \\; \\bar{x}$. However, plugging this into the derivative with respect to $\\alpha$ yields:\n",
    "\n",
    "$$\\frac{\\partial log(l)(\\alpha,\\beta)}{\\partial \\alpha} = \\log(\\alpha) + \\overline{\\log(x)} - \\log(\\bar{x}) - \\frac{\\partial log(\\Gamma(\\alpha))}{\\partial\\alpha} = 0$$\n",
    "\n",
    "This has ***no closed form solution***! We must use ***numerical optimization***!\n",
    "\n",
    "Numerical optimization algorithms take an initial \"*guess*\" at the solution, and iteratively improves the guess until it gets \"*close enough*\" to the answer.\n",
    "\n",
    "Here, we will use [Newton-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method) algorithm from [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.newton.html):\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a graphical example of how Newton-Raphson converges on a solution: what is the $x$ for which  $f(x) = 0$, using derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function\n",
    "func = lambda x: 3./(1 + 400*np.exp(-2*x)) - 1\n",
    "xvals = np.linspace(0, 6)\n",
    "plt.plot(xvals, func(xvals))\n",
    "plt.text(5.3, 2.1, '$f(x)$', fontsize=16)\n",
    "\n",
    "# zero line\n",
    "plt.plot([0,6], [0,0], 'k-')\n",
    "\n",
    "# value at step n\n",
    "plt.plot([4,4], [0,func(4)], 'k:')\n",
    "plt.text(4, -.2, '$x_n$', fontsize=16)\n",
    "\n",
    "# tangent line\n",
    "tanline = lambda x: -0.858 + 0.626*x\n",
    "plt.plot(xvals, tanline(xvals), 'r--')\n",
    "\n",
    "# point at step n+1\n",
    "xprime = 0.858/0.626\n",
    "plt.plot([xprime, xprime], [tanline(xprime), func(xprime)], 'k:')\n",
    "plt.text(xprime+.1, -.2, '$x_{n+1}$', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_n$ be our current estimate. Then the next estimate $x_{n+1}$ is obtained as follows: Draw the tangent line at $(x_n,f(x_n))$. Then $x_{n+1}$ is the point where the tangent line meets the x-axis. That tangent line meets the x-axis at a point often much closer to the root of the curve than $x_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the Newton-Raphson algorithm, we need a function that returns a vector containing the **first and second derivatives** of the function with respect to the variable of interest (the second equation above, setting the derivative with respect to $\\alpha$ to zero). \n",
    "\n",
    "`psi` and `polygamma` are complex functions of the Gamma function that result when you take first and second derivatives of that function. Specifically, \n",
    "\n",
    "[scipy.special.psi](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.psi.html) = $\\frac{\\partial log(\\Gamma(\\alpha))}{\\partial\\alpha}$ and \n",
    "\n",
    "[scipy.special.polygamma](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.polygamma.html) = $\\frac{\\partial^2 log(\\Gamma(\\alpha))}{\\partial\\alpha^2}$\n",
    "\n",
    "and the derivative of the second equation above is:\n",
    "\n",
    "$$\\frac{\\partial^2 log(l)(\\alpha,\\beta)}{\\partial \\alpha^2} = \\frac{1}{\\alpha} - \\frac{\\partial^2 log(\\Gamma(\\alpha))}{\\partial\\alpha^2}$$\n",
    "\n",
    "Then are our fist and second derivatives of the second equation above, spelled out in python, are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import psi, polygamma\n",
    "\n",
    "# Note that log_mean and mean_log are parameters of the dlgamma function\n",
    "dlgamma = lambda m, log_mean, mean_log: np.log(m) - psi(m) - log_mean + mean_log\n",
    "dl2gamma = lambda m, *args: 1./m - polygamma(1, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `log_mean` and `mean_log` are $\\log{\\bar{x}}$ and $\\overline{\\log(x)}$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "log_mean = precip.mean().apply(np.log)\n",
    "mean_log = precip.apply(np.log).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use our `newton` function to find the value of $\\alpha$ for which $$\\frac{\\partial log(l)(\\alpha,\\beta)}{\\partial \\alpha} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha MLE for December\n",
    "alpha_mle = newton(dlgamma, 2, dl2gamma, args=(log_mean[-1], mean_log[-1]))\n",
    "alpha_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now plug this back into the solution for beta:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$ \\beta  = \\frac{\\alpha}{\\bar{X}} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_mle = alpha_mle/precip.mean()[-1]\n",
    "beta_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the fit of the estimates derived from MLE to those from our method of moments (MOM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_mean = precip.mean()\n",
    "precip_mean\n",
    "\n",
    "precip_var = precip.var()\n",
    "precip_var\n",
    "\n",
    "alpha_mom = precip_mean ** 2 / precip_var\n",
    "beta_mom = precip_var / precip_mean\n",
    "alpha_mom, beta_mom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot both models and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import gamma\n",
    "\n",
    "dec = precip.Dec\n",
    "dec.hist(normed=True, bins=10, grid=False)\n",
    "x = np.linspace(0, dec.max())\n",
    "plt.plot(x, gamma.pdf(x, alpha_mom[-1], beta_mom[-1]), 'g-')\n",
    "plt.plot(x, gamma.pdf(x, alpha_mle, beta_mle), 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooooooh... The red dotted curve is a *better model* of our data than the continuous mauve (purple) curve. So MLE gives us a ***better model*** than MOM!\n",
    "\n",
    "For common distributions, `SciPy` includes methods for fitting via MLE, if you assume a certain pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "\n",
    "gamma.fit(precip.Dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fit is not directly comparable to our estimates, however, because SciPy's `gamma.fit` method fits an odd 3-parameter version of the gamma distribution. But don't let the three parameters  (alpha, loc, beta) provided throw you off! There's `alpha` and `beta` that you know of, and the other one is `loc`, the location at which the gamma distribution ***starts growing***, since you can liberally move the gamma along the x-axis.\n",
    "\n",
    "In general, the gamma function has three different parametrizations (so you need to make sure the API you use has the parametrization you expect):\n",
    "\n",
    "- With a shape parameter k and a scale parameter θ.\n",
    "- With a shape parameter α = k and an inverse scale parameter β = 1/θ, called a rate parameter.\n",
    "- With a shape parameter k and a mean parameter μ = k/β.\n",
    "\n",
    "I already told you last notebook that the parametric gamma function has two possible expressions, using $\\alpha$ and $\\beta$, or $k$ and $\\theta$.\n",
    "\n",
    "It's possible to shift and/or scale the distribution using the loc and scale parameters. Specifically, gamma.pdf(x, alfa, loc, scale) is identically equivalent to gamma.pdf(y, alfa) / scale with y = (x - loc) / scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Another dataset example\n",
    "\n",
    "Let's try our hands at another dataset that better illustrates the modeling process because it incorporates an improvement on the model.\n",
    "\n",
    "We'll use `statsmodels`, a Python package for statistical data analyses, which will help us minimize the math!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/happy-cat.jpg width = 400 />\n",
    "    Less math baby!\n",
    "</center>\n",
    "\n",
    "## Lab \\#2: Hearts dataset\n",
    "\n",
    "`statsmodels` just like `R`, also contains real-world datasets that you can use to experiment with new methods. Let's load the **heart dataset** from `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import statsmodels.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data = statsmodels.datasets.heart.load_pandas().data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains censored and uncensored data: a censor of 0 means that the patient was ***alive at the end of the study***, and thus we don't know the exact survival time. We only know that the patient survived at least the indicated number of days. \n",
    "\n",
    "Let's only keep uncensored data (thus introduce a [bias](https://en.wikipedia.org/wiki/Bias) toward patients that ***did not survive very long*** after their transplant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.censors == 1]\n",
    "survival = data.survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data graphically, by plotting the raw survival data and the histogram. Notice how the data is 2D, however the histogram is really a 1D function: It gives us the possible $y$'s and their ***distribution***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax1.plot(sorted(survival)[::-1], 'o')\n",
    "ax1.set_xlabel('Patient')\n",
    "ax1.set_ylabel('Survival time (days)')\n",
    "\n",
    "ax2.hist(survival, bins=15)\n",
    "ax2.set_xlabel('Survival time (days)')\n",
    "ax2.set_ylabel('Number of patients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the histogram is decreasing very rapidly! Patients died *fast*!\n",
    "\n",
    "Eyeballing the data, let's try to fit an [exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution) to the data. Isn't that would *you* would try?\n",
    "\n",
    "According to the exponential model, S (number of days of survival) is an exponential random variable with the parameter λ, and the observations $s_i$ are sampled from this distribution. Let the sample **mean** be:\n",
    "\n",
    "$$\\overline s = \\frac 1 n \\sum s_i$$\n",
    "\n",
    "The likelihood function of an exponential distribution is as follows:\n",
    "\n",
    "$$\\mathcal{L}(\\lambda, \\{s_i\\}) = P(\\{s_i\\} \\mid \\lambda) = \\lambda^n \\exp\\left(-\\lambda n \\overline s\\right)$$\n",
    "\n",
    "Here's the proof:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\lambda, \\{s_i\\}) &= P(\\{s_i\\} \\mid \\lambda) &\\\\\n",
    "&= \\prod_{i=1}^n P(s_i \\mid \\lambda) & \\textrm{(by independence of the $s_i$)}\\\\\n",
    "&= \\prod_{i=1}^n \\lambda \\exp(-\\lambda s_i) &\\\\\n",
    "&= \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^n s_i\\right) &\\\\\n",
    "&= \\lambda^n \\exp\\left(-\\lambda n \\overline s\\right) &\n",
    "\\end{align*}\n",
    "\n",
    "Here, $\\overline s$ is the sample mean.\n",
    "\n",
    "To find the maximum of this function, let's compute its derivative function with respect to $λ$:\n",
    "\n",
    "$$\\frac{d\\mathcal{L}(\\lambda, \\{s_i\\})}{d\\lambda} = \\lambda^{n-1} \\exp\\left(-\\lambda n \\overline s \\right) \\left( n - n \\lambda \\overline s \\right)$$\n",
    "\n",
    "The root of this derivative is therefore $λ=1\\;/\\;\\overline s$. We're lucky here, the exponential is simple to diffferentiate.  In more complex situations, we would require numerical optimization methods, like Newton-Raphson above, to maximize the likelihood function.\n",
    "\n",
    "Let's compute this parameter ***numerically***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smean = survival.mean()\n",
    "rate = 1. / smean\n",
    "rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the fitted exponential distribution to the data. We first need to generate linearly spaced values for the x-axis (days):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smax = survival.max()\n",
    "days = np.linspace(0., smax, 1000)\n",
    "# bin size: interval between two consecutive values in `days`\n",
    "dt = smax / 999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the probability density function of the exponential distribution with` SciPy`. \n",
    "\n",
    "The parameter is the scale, the inverse of the estimated rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_exp = st.expon.pdf(days, scale=1. / rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the histogram and the obtained distribution. \n",
    "\n",
    "We need to rescale the theoretical distribution to the histogram (depending on the bin size and the total number of data points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 30\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.hist(survival, nbins)\n",
    "ax.plot(days, dist_exp * len(survival) * smax / nbins,\n",
    "        '-r', lw=3)\n",
    "ax.set_xlabel(\"Survival time (days)\")\n",
    "ax.set_ylabel(\"Number of patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... \n",
    "\n",
    "What do you think?\n",
    "\n",
    "The fit between the model and our data is ***ok-ish***, but not ***perfect***.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 A better model\n",
    "\n",
    "Just like with our probability function `p`, you can avoid the math when you can write python! \n",
    "\n",
    "You can also avoid the math if you know how to write python and you know where to find the useful libraries!\n",
    "\n",
    "`SciPy` actually integrates numerical maximum likelihood routines for a large number of distributions. Let's leverage `SciPy` to estimate the parameter of the exponential distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = st.expon\n",
    "args = dist.fit(survival)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we plot, let's perform a **goodness of fit test**. \n",
    "\n",
    "A good statistical goodness of fit test is the [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test), also known as **KS** test. it is sensitive to differences in *both* location and shape of the empirical [cumulative distribution functions](https://en.wikipedia.org/wiki/Cumulative_distribution_function) (cdf) of the two samples.\n",
    "\n",
    "The cdf is the area under the pdf: the cdf evaluated at $x$, is the probability that $X$ will take a value less than or equal to $x$. In the case of scalar continuous distributions, it gives the area under the pdf from minus infinity to $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.kstest(survival, dist.cdf, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `pvalue` is very low: the null hypothesis (stating that the observed data stems from an exponential distribution with a maximum likelihood rate parameter) can be rejected with high confidence!\n",
    "\n",
    "- We'll talk about p-values and null-hypotheses in our ***next lecture***. They're important point estimators in classical statistical inference. Still very much used in industry (but not as powerful as *Bayesian inference*, which yields pdfs instead of point estimates).\n",
    "\n",
    "*Oh no*...! The ***KS*** test says that our exponential distribution is thus ***not a good fit for the data***!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another distribution, the [Birnbaum-Sanders distribution](https://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution), which is typically used to model **failure times**. You wouldn't know about this model distribution ***without some experience in data science***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = st.fatiguelife\n",
    "args = dist.fit(survival)\n",
    "st.kstest(survival, dist.cdf, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the `pvalue` is about 0.073, so that we would ***not reject the null hypothesis*** with a five percent confidence level!\n",
    "\n",
    "Ok, let's plot now to see if we get a better fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_fl = dist.pdf(days, *args)\n",
    "nbins = 30\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.hist(survival, nbins)\n",
    "ax.plot(days, dist_exp * len(survival) * smax / nbins,\n",
    "        '-r', lw=3, label='exp')\n",
    "ax.plot(days, dist_fl * len(survival) * smax / nbins,\n",
    "        '--g', lw=3, label='BS')\n",
    "ax.set_xlabel(\"Survival time (days)\")\n",
    "ax.set_ylabel(\"Number of patients\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Birnbaum-Sanders (BS) fits the data a lot better than the exp distribution!\n",
    "\n",
    "How did I know about the BS distribution? I googled. And I thought it was very funny that it was called the ***BS distribution*** :-)\n",
    "\n",
    "Once again, \n",
    "\n",
    "- The maximum likelihood estimate (MLE) for the rate parameter(s) is, by definition, the value of the parameters that maximizes the likelihood function. It is the parameter(s) that maximize the probability of observing the data, assuming that the observations are actually sampled from the distribution we picked as a model.\n",
    "\n",
    "We then either verify by plotting the data and the model with the MLE parameter(s) and see if it's a good match, or use goodness of fit tests like the **KS** test to get a more objective estimate. There are [many](https://en.wikipedia.org/wiki/Goodness_of_fit) different goodness of fit tests! Which we pick is part of the art!\n",
    "If we are wrong and the fit is not very good, back to the drawing board for another model!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/frustration.png width = 400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you have a correct data model..\n",
    "\n",
    "You can keep the model and its parameters, and ***throw away the data***. \n",
    "\n",
    "To generate a new value of the pseudorandom variable, use a series of invocations of the `.rvs()` method for your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Kernel Density Estimates\n",
    "\n",
    "You can also estimate a probability distribution ***nonparametrically*** using [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation.) (KDE). That is actually how `seaborn` plots the function\n",
    "best matching a histogram.\n",
    "\n",
    "In some instances, you may not be interested in the parameters of a particular distribution of data, but just a smoothed representation of the data at hand. In this case, you can estimate the disribution *non-parametrically* (i.e. making no assumptions about the form of the underlying distribution) using [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE). \n",
    "\n",
    "KDE is a fundamental data smoothing problem where inferences about the population are made based on a finite data sample.\n",
    "\n",
    "The kernel density estimator of a set of n points ${x_i}$ is given as:\n",
    "\n",
    "$$\\hat{f}_h(x) = \\frac{1}{nh} \\sum_{i=1}^n K\\Big(\\frac{x-x_i}{h}\\Big)$$\n",
    "\n",
    "Here, $h>0$ is a scaling parameter (the bandwidth) and $K(u)$ is the kernel, a symmetric function that integrates to 1. This estimator is to be compared with a classical histogram, where the kernel would be a top-hat function (a rectangle function taking its values in ${0,1}$), but the blocks would be located on a regular grid instead of the data points.\n",
    "\n",
    "Multiple kernels can be chosen. Here, we chose a Gaussian kernel, so that the KDE is the superposition of Gaussian functions centered on all the data points. It is an estimation of the density.\n",
    "\n",
    "The choice of the bandwidth is not trivial; there is a [tradeoff](https://en.wikipedia.org/wiki/Bias-variance_dilemma.) between a too low value (small bias, high variance: overfitting) and a too high value (high bias, small variance: underfitting). \n",
    "\n",
    "There are several methods to automatically choose a sensible bandwidth. SciPy uses a rule of thumb called Scott's Rule: $h = n^{\\frac{-1}{d + 4}}$\n",
    "\n",
    "The following figure illustrates the KDE. The sample dataset contains four points in [0,1] (black lines). The estimated density is a smooth curve, represented here with different bandwidth values.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/kde.png width = 600 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random data\n",
    "y = np.random.random(50) * 10\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, bins=16, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import norm\n",
    "\n",
    "x = np.linspace(0, 10, 100)\n",
    "\n",
    "# Smoothing parameter\n",
    "s = 0.4\n",
    "\n",
    "# Calculate the kernels\n",
    "kernels = np.transpose([norm.pdf(x, yi, s) for yi in y])\n",
    "\n",
    "#plt.plot(x, y)\n",
    "plt.plot(x, kernels, 'k:')\n",
    "plt.plot(x, kernels.sum(1))\n",
    "plt.plot(y, np.zeros(len(y)), 'ro', ms=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy implements a Gaussian KDE that automatically chooses an appropriate bandwidth. \n",
    "\n",
    "## 4.2 Bimodal distribution\n",
    "\n",
    "Let's create a **bi-modal** (two-humped) distribution of data that is not easily summarized by a known parametric distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bi-modal distribution with a mixture of Normals.\n",
    "x1 = np.random.normal(0, 3, 50)\n",
    "x2 = np.random.normal(4, 1, 50)\n",
    "\n",
    "# Append by row\n",
    "x = np.r_[x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x, bins=8, normed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use kde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kde\n",
    "\n",
    "density = kde.gaussian_kde(x)\n",
    "xgrid = np.linspace(x.min(), x.max(), 100)\n",
    "plt.hist(x, bins=8, normed=True)\n",
    "plt.plot(xgrid, density(xgrid), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooooh... Nice fit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 A spatial distribution (2D)\n",
    "\n",
    "We look at the geographical locations of tropical cyclones from 1848 to 2013, based on data provided by the NOAA, the US' National Oceanic and Atmospheric Administration.\n",
    "\n",
    "We use a kernel density estimation (KDE) to estimate that pdf.\n",
    "\n",
    "First, install a [map server](https://pypi.org/project/geos/):\n",
    "```(python)\n",
    "pip install geos\n",
    "```\n",
    "\n",
    "If you want a full geometry engine (*optional*), look [here](https://trac.osgeo.org/geos).\n",
    "\n",
    "Then, import the [cartopy](http://scitools.org.uk/cartopy/) library:\n",
    "```(python)\n",
    "conda install cartopy\n",
    "```\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/cartopy-install.png width = 900 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import cartopy.crs as ccrs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Open the data with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data\n",
    "df = pd.read_csv('data/Allstorms.ibtracs_wmo.v03r05.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains information about most storms since 1848. A single storm may appear multiple times across several consecutive days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[[0, 1, 3, 8, 9]]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas' `groupby()` function to obtain the average location of every storm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.groupby('Serial_Num')\n",
    "pos = dfs[['Latitude', 'Longitude']].mean()\n",
    "x = pos.Longitude.values\n",
    "y = pos.Latitude.values\n",
    "pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the storms on a map with cartopy! \n",
    "\n",
    "This toolkit allows us to easily project the geographical coordinates on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a simple equirectangular projection,\n",
    "# also called Plate Carree.\n",
    "geo = ccrs.Geodetic()\n",
    "crs = ccrs.PlateCarree()\n",
    "# We create the map plot.\n",
    "ax = plt.axes(projection=crs)\n",
    "# We display the world map picture.\n",
    "ax.stock_img()\n",
    "# We display the storm locations.\n",
    "ax.scatter(x, y, color='r', s=.5, alpha=.25, transform=geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before performing the kernel density estimation, we transform the storms' positions from the **geodetic coordinate system** (longitude and latitude) into the map's coordinate system, called **plate carrée**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = crs.transform_points(geo, x, y)[:, :2].T\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform the kernel density estimation on our (2, N) array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = st.gaussian_kde(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gaussian_kde()` routine returned a Python function. To see the results on a map, we need to evaluate this function on a 2D grid spanning the entire map. We create this grid with meshgrid(), and we pass the x and y values to the kde() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "# Coordinates of the four corners of the map.\n",
    "x0, x1, y0, y1 = ax.get_extent()\n",
    "# We create the grid.\n",
    "tx, ty = np.meshgrid(np.linspace(x0, x1, 2 * k),\n",
    "                     np.linspace(y0, y1, k))\n",
    "# We reshape the grid for the kde() function.\n",
    "mesh = np.vstack((tx.ravel(), ty.ravel()))\n",
    "# We evaluate the kde() function on the grid.\n",
    "v = kde(mesh).reshape((k, 2 * k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before displaying the KDE heatmap on the map, we need to use a special colormap with a transparent channel. This will allow us to superimpose the heatmap on the stock image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [https://stackoverflow.com/a/37334212/1595060](https://stackoverflow.com/a/37334212/1595060)\n",
    "cmap = plt.get_cmap('Reds')\n",
    "my_cmap = cmap(np.arange(cmap.N))\n",
    "my_cmap[:, -1] = np.linspace(0, 1, cmap.N)\n",
    "my_cmap = ListedColormap(my_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we display the estimated density with imshow():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=crs)\n",
    "ax.stock_img()\n",
    "ax.imshow(v, origin='lower',\n",
    "          extent=[x0, x1, y0, y1],\n",
    "          interpolation='bilinear',\n",
    "          cmap=my_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooooh! What a nice model! And... no math!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion: Limitations of MLE and classical statistical estimation\n",
    "\n",
    "What do you ***need to know***, from classical statistics, to succeed in your data science interview?\n",
    "\n",
    "- Data science is about building a model from the data so that you can throw away the data (which moreover also has noise that we don't want), and use the model to reproduce the data and do predictions. Example: Weather people!\n",
    "\n",
    "- MOM equates the empirical and theoretical moments to yield the parameters of your model. \n",
    "\n",
    "- MLE gives you the value which maximises the Likelihood P(D|θ). \n",
    "\n",
    "- [Maximum a Posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) (MAP), which we skipped over but you can read about in wikipedia, yields the value which maximises the posterior probability $P(θ\\;|\\;D)$. As both methods give you a single fixed value, they’re considered to be **point estimators**.\n",
    "\n",
    "- How to numerically evaluate the parameters of your model using the first two methods. You *do not need to know the math* (but if you understand it, that is a good thing!)\n",
    "\n",
    "**Bayesian inference**, as we will see soon, ***fully calculates the posterior probability distribution***, as Bayes' formula below. \n",
    "\n",
    "$$p(θ \\; | \\; D) = \\frac{p(D \\; | \\; θ) \\; p(θ)}{p(D)}$$\n",
    "\n",
    "Hence the output is not a single value, i.e. a **point estimate** for the parameters of your model, but a **probability density function** (when θ is a continuous variable) or a ***probability mass function*** (when θ is a discrete variable) ***for the parameters of your model***. \n",
    "\n",
    "That way, you know what the most likely value is, ***but also the amount of error you might be making***!\n",
    "\n",
    "Ooooooh....\n",
    "\n",
    ">**Super important**: MLE and MAP return a single fixed value(s) for the model parameter(s), Bayesian inference returns functions (pdfs)  instead!\n",
    "\n",
    "For a preview of Bayesian statistical estimation, here's an example:\n",
    "\n",
    "Assume you’re in a casino with full of slot machines with 50% winning probability. After playing for a while, you hear a rumour that there’s ***one special slot machine*** with 67% winning probability! But you don't know which one it is (yet)!\n",
    "\n",
    "Now, you’re observing people playing at 2 suspicious slot machines (you’re sure that one of those is the special slot machine!) and get the following data.\n",
    "\n",
    "Machine A: 3 wins out of 4 plays\n",
    "Machine B: 81 wins out of 121 plays\n",
    "\n",
    "By intuition, you would think machine B is the special one! Because 3 wins out of 4 plays on machine A could just happen by chance. But machine B’s data doesn’t look like it's happening by chance!\n",
    "\n",
    "The posterior probability distribution P(θ|D), calculated as a Gamma function, is plotted below for the two machines:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/tricked-slot-machines.png width = 400 />\n",
    "</center>\n",
    "\n",
    "Although both distributions have their mode on θ=0.666 (MLE estimate), the shapes of the distributions are quite different. Density around the mode is much higher in the distribution of machine B than the one of machine A. *We know more about Machine B than about Machine A!*\n",
    "\n",
    "So a pdf yields much more information than a point estimate. In particular, it tells us about errors in the estimation. And that is very important when a model is making a potentially dangerous prediction (e.g. ***Tesla: Yup, I'm pretty sure you can drive 120 mph on this road***).\n",
    "\n",
    "***Ummm... Tesla computer, what's an estimation of the error you might be making in your prediction?***\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/tesla-model-3.jpg width = 300 />\n",
    "</center>\n",
    "\n",
    "So we will now turn our attention to **Bayesian estimation** of our models, rather than **point estimates**. \n",
    "\n",
    "Bayes' formula is actually pretty complex. Specifically, the term in the denominator, a **marginal probability**,  needs to be calculated for every possible θ:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/bayes-complex.png width = 350 />\n",
    "</center>\n",
    "\n",
    "That integral sum (for the continuous case, a simple sun for discrete cases) is the reason why we had to wait for powerful laptops before we could actually put Bayesian estimation to practice. \n",
    "\n",
    "When the model is ***analytic***, like the ones in this notebook, solutions are forthcoming, albeit with a bit of math. But that is rarely the case in real-world applications. We then need to use ***Monte Carlo*** and other probabilistic programming methodologies as substitute for direct integral computation. That is what we'll look at next week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
