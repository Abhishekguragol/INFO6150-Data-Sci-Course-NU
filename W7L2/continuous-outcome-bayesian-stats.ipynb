{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 7 Day 2</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 20 October 2022, with material from Peter Norvig and Chris Fonnesbeck</div>\n",
    "\n",
    "# Bayesian statistical analysis and probabilistic programming\n",
    "\n",
    "We now know how to use analytical parametric pdfs with known properties, and to match them with empirical data in order to zero in on the right parameters. It involves a bit of math (MOM), or a lot of math (MLE) for better estimates. The parameters are **point estimates**, the **data model** is **frequentist**, and we have little idea of the error we're making. The KS test does give us an idea, but how do we know where to draw the line on the p-value?\n",
    "\n",
    "Today, we'll detail an **algorithmic approach** and a **Bayesian model** to do the same thing, but we'll also get a ***much better estimate of the error we're making*** in picking our parameters. Next week, we'll also *explore* that algorithm.\n",
    "\n",
    "A frequentist model is described by an analytic function and its parameters. The method for solving for the model gives us what the most likely values for the parameters. MOM does this. MLE does this, even better. \n",
    "\n",
    "A **Bayesian method** yields parameters too, but it also yields **uncertainty** about those parameters. \n",
    "\n",
    "In other words, the model is described as a probability distribution, just like frequentist models, but the uncertainly in its parameters is *also* described as probability distributions. How wild is that?! The model is a pdf, and its parameters are *also* pdfs!\n",
    "\n",
    "Run the cell below, we'll use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian vs Frequentist Statistics: *What's the difference?*\n",
    "\n",
    "*Any* statistical inference scheme, Bayesian or otherwise, involves at least the following: \n",
    "\n",
    "1. Some **unknown quantities**, which we are interested in learning or predicting. These are called the **dependent variables**\n",
    "2. Some **data** which have been observed, and hopefully contains information leading to the dependent variables. These are called the **independent variables**. Note that some of these may be **correlated** with themselves (linearly or not), so we should be able to throw the correlated ones and only use the ***really independent variables*** for predicting the dependent ones\n",
    "3. One (or more) **models** that relate the independent variables to the dependent variables via a probablity distribution function (pdf). The pdf will yield **variates** that essentialy statistically ***look like the real data***. \n",
    "\n",
    "The model is the instrument you use to **learn** about the underlying process that yields the data. For example, you learn about the real world from the model that your parents build for you then teach you, before you leave home to build your own models. Machines build models to learn, too. They either learn them from the data, or we (humans) can also teach them the model, like parents to them! For example, we have meteorological models, which predict weather.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/robot-daddy.jpg\" width=\"400\" />\n",
    "</center>\n",
    "\n",
    "In a **Frequentist** world view, **data** observed is considered **random**, because it is the realization of random processes and hence will vary each time one goes to observe the system. Model **parameters** are considered **fixed**. A parameter's true value may be as of yet unknown, but it's fixed. You need to wait till you have *all the data* before you can use methods to evaluate it.\n",
    "\n",
    "For example, Max Verstappen's probability of winning is fixed for the season. That's what you did in your homework. Jesus Christ is a central parameter in the Christian World Model. Christians will say the world order may be random because of human misgivings, but Jesus Christ and his compassion (the parameter) is fixed and steadfast.\n",
    "\n",
    "In a frequentist world view, we take the winning scores of Lewis Hamilton from last season, and use these parameters as the ground truth for probabilities of wins this season. The wins may not match predictions, but that is because the world is **random**.\n",
    "\n",
    "In a **Bayesian** world view,  *data* is considered **fixed**. Model parameters are **random** and treated as probability distributions. Model parameters *change all the time*, and as soon as you observe new evidence, you need to re-evaluate them!\n",
    "\n",
    "For example, we need to reevaluate Max Verstappen's probability of winning *after every race*. I'm sur eyou felt that, too, when you did your homework. some Christians may postulate that world order is predetermined, however Jesus Christ's compassion may vary because.. *sometimes he gets exasperated by his followers*!\n",
    "\n",
    "This religious analogy is *mine*, so if it's not very good, I apologize in advance! In my simple mind, it helps me understand what is *fixed*, and what is *variable*.\n",
    "\n",
    "In a Bayesian world view, the world is not random: There is a perfect scientific explanation for everything. However, the probabilities for MV winning a race ***changes*** after every race. Sometimes he's on a winning streak, and other times on a losing streak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Formula\n",
    "\n",
    "While frequentist statistics uses different estimators for different problems, Bayes formula is the **only estimator** that Bayesians need to obtain estimates of unknown quantities. \n",
    "\n",
    "The equation expresses how our belief about the value of \\\\(\\theta\\\\) (the parameter), as expressed by the **prior distribution** \\\\(P(\\theta)\\\\) is reallocated following the observation of the data \\\\(y\\\\). \n",
    "\n",
    "For **discrete random variables**:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "\\\\[Pr(\\theta\\;|\\;y) = \\frac{Pr(\\theta \\cap y)}{Pr(y)} = \\frac{Pr(y\\;|\\;\\theta)Pr(\\theta)}{\\sum_\\theta Pr(y\\;|\\;\\theta)Pr(\\theta)} \\\\]\n",
    "</div>\n",
    "\n",
    "The denominator is actually the expression in the numerator integrated over all possible discrete model parameters \\\\(\\theta\\\\).\n",
    "\n",
    "For **continuous random variables**, the denominator usually cannot be computed directly:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "\\\\[Pr(\\theta\\;|\\;y) = \\frac{Pr(y\\;|\\;\\theta)Pr(\\theta)}{\\int Pr(y\\;|\\;\\theta)Pr(\\theta) d\\theta}\\\\]\n",
    "</div>\n",
    "\n",
    "The denominator is the expression in the numerator integrated over all possible continuous model parameters \\\\(\\theta\\\\)\n",
    "\n",
    "The **intractability** of the integral in the denominator was the reason for the under-utilization of Bayesian methods by statisticians for many years. But with the advent of computers and clever algorithms like [Metropolis-Hastings](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm), this has changed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M&Ms\n",
    "Remember this?\n",
    "\n",
    "> The blue M&M was introduced in 1995.  Before then, the color mix in a bag of plain M&Ms was (30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, 10% Tan).  Afterward it was (24% Blue , 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown). \n",
    "A friend of mine has two bags of M&Ms, and he tells me that one is from 1994 and one from 1996.  He won't tell me which is which, but he gives me one M&M from each bag.  One is yellow and one is green.  What is the probability that the yellow M&M came from the 1994 bag? Well, the old M&M bags' yellow count was higher, so it must be higher, right? But how to count?\n",
    "\n",
    "In a frequentist world view, you pick M&Ms the way we picked balls from our urns in our first lecture on probabilities. We just count all favorable outcomes and divide by all outcomes. No new experiment will change anything about that.\n",
    "\n",
    "In a Bayesian world view, experiments change *everything*.\n",
    "\n",
    "We are asked about the probability of an event (yellow_M&M94 + green_M&M96) given the evidence (M&M94 + M&M96 = {yellow, green}). The probability of the event is not readily available. However the probability of the evidence, given the event, is readily available!  \n",
    "\n",
    "***Before*** (prior) we see the colors of the M&Ms, there are two hypotheses, `A` and `B`, both with equal probability:\n",
    "\n",
    "    A: first M&M from 94 bag, second from 96 bag\n",
    "    B: first M&M from 96 bag, second from 94 bag\n",
    "    P(A) = P(B) = 0.5\n",
    "    \n",
    "***Then*** (posterior) we get some evidence:\n",
    "    \n",
    "    E: first M&M yellow, second green\n",
    "    \n",
    "We want to know the ***new*** (posterior) probability of hypothesis `A`, given the evidence:\n",
    "    \n",
    "    P(A | E)\n",
    "    \n",
    "That's not easy to calculate, except by enumerating the sample space (frequentist world view). But Bayes Theorem says:\n",
    "    \n",
    "    P(A | E) = P(E | A) * P(A) / P(E)\n",
    "    \n",
    "The quantities on the *right-hand-side* are easier to calculate:\n",
    "    \n",
    "    P(E | A) = 20/100 * 20/100 = 0.04\n",
    "    P(E | B) = 10/100 * 14/100 = 0.014\n",
    "    P(A)     = 0.5\n",
    "    P(B)     = 0.5\n",
    "    P(E)     = P(E | A) * P(A) + P(E | B) * P(B) \n",
    "             = 0.04     * 0.5  + 0.014    * 0.5   =   0.027\n",
    "             \n",
    "Where did the probability of the evidence P(E) formula come from?\n",
    "\n",
    "There are two possibilities of getting the evidence: A and B, a *union* and so we sum their probabilities. The joint probability of the evidence *and* case A is a succession or *intersection*, so it must be a product of their probabilities: P(E|A).P(A). Likewise for the case B: P(E|B).P(B) \n",
    "    \n",
    "And so here is how Bayes helps us get a final answer:\n",
    "    \n",
    "    P(A | E) = P(E | A) * P(A) / P(E) \n",
    "             = 0.04     * 0.5  / 0.027 \n",
    "             = 0.7407407407\n",
    "             \n",
    "This is the **posterior** probability that A came from the '94 bag, given an event, which changed our world view that the probability was 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In search of the model parameter $\\theta \\;$\n",
    "\n",
    "Suppose we are given some data and we are told that there is a process that yields this data, and which we must try to model. So we must \n",
    "- 1: Pick the right pdf from the catalogue, and \n",
    "- 2: Determine the right $\\theta$s for the data. \n",
    "\n",
    "More specifically, we are concerned with *beliefs* about what the $\\theta$s might be: Rather than guessing the $\\theta$s exactly, we talk about what $\\theta$s are ***likely to be*** by assigning a probability distribution to them!\n",
    "\n",
    "But these probability distributions are hidden from us. We see only see the data, and must ***go backwards*** to try and determine the $\\theta$s to build the best possible model of our data. This problem is **difficult** because there is no one-to-one mapping from the data to the $\\theta$s. \n",
    "\n",
    "In classical statistics we use the Method of Moments (MOM), and Maximum Likelihood Estimation (MLE), to get **point estimates** (not pdfs) for $\\theta$.\n",
    "\n",
    "In the Bayesian approach, we use **probabilistic programming** to solve this problem, with **Markov Chain Monte Carlo** (MCMC) methods and variational inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Bayesian Model for 2 groups with continuous outcome\n",
    "\n",
    "Let's do statistical inference for two groups (two statistics), with continuous random variables. \n",
    "\n",
    "We'll use the fictitious example from [Kruschke (2012)](http://www.indiana.edu/~kruschke/articles/KruschkeAJ2012.pdf) concerning the evaluation of a clinical trial for drug evaluation. The trial aims to evaluate the efficacy of a \"*smart drug*\" that is supposed to increase intelligence by comparing IQ scores of individuals in a treatment arm (those receiving the drug) to those in a control arm (those recieving a placebo). There are 47 individuals and 42 individuals in the treatment (`drug`) and control (`placebo`) arms, respectively, and these are their post-trial IQs. An IQ between 90 and 110 is considered average; over 120, superior. Let's look at the histograms of our data, first thing you should always do.\n",
    "\n",
    "Note that although our IQ data is integer type, our datasets here could easily be real-valued, and so we consider our random variable to be continuous.\n",
    "\n",
    "Please plot histograms using `pd.concat([drug, placebo], ignore_index=True)`, and then `.hist('iq', by='group')` on the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = pd.DataFrame(dict(iq=(101,100,102,104,102,97,105,105,98,101,100,123,105,103,100,95,102,106,\n",
    "        109,102,82,102,100,102,102,101,102,102,103,103,97,97,103,101,97,104,\n",
    "        96,103,124,101,101,100,101,101,104,100,101),\n",
    "                         group='drug'))\n",
    "placebo = pd.DataFrame(dict(iq=(99,101,100,101,102,100,97,101,104,101,102,102,100,105,88,101,100,\n",
    "           104,100,100,100,101,102,103,97,101,101,100,101,99,101,100,100,\n",
    "           101,100,99,101,100,102,99,100,99),\n",
    "                            group='placebo'))\n",
    "\n",
    "trial_data = pd.concat([drug, placebo], ignore_index=True)\n",
    "trial_data.hist('iq', by='group')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Student-T distribution\n",
    "\n",
    "Hmm... there appears to be extreme (\"outlier\") values in the data (bins on the left and/or right that are distant from where most of the data lies, we say the data \"*has wings*). That is a good indicator to pick a new pdf from our catalogue of all possible pdfs, called the [Student-t](https://en.wikipedia.org/wiki/Student%27s_t-distribution) distribution, to describe the distributions of the scores in each group. \n",
    "\n",
    "It was developed by [William Sealy Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset) under the pseudonym `Student`. That's because William worked for Guiness, and Guiness was very worried about its secret Beer formula. Another researcher at Guinness had previously published a paper containing trade secrets of the Guinness brewery. To prevent further disclosure of confidential information, Guinness prohibited its employees from publishing any papers regardless of the contained information. So William published his results with a pseudonym. \n",
    "\n",
    "Another researcher, [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) introduced a new form of that statistic, denoted `t`. The t-form was adopted because it fit in with Fisher's theory of degrees of freedom. And so now we're stuck with the mysteriously-named `Student-t` distribution, which works great modeling histograms ***with outliers*** (like financial data!).\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/guiness.jpg\" width=200 />\n",
    "</center>\n",
    "\n",
    "This sampling distribution adds **robustness** to the analysis, as a `T distribution` is less sensitive to outlier observations, relative to a `normal` distribution. In other words, if you have ***a lot of outliers*** in your data and attempt to model your data with a normal distribution, your outliers will *skew* your model so that it does not fit the non-outlier data very well.\n",
    "\n",
    "The **three-parameter** Student-t distribution allows for the specification of the following $\\theta$s: A mean $\\mu$, a precision (inverse-variance) $\\lambda$, and a degrees-of-freedom parameter $\\nu$:\n",
    "\n",
    "$$f(x\\;|\\;\\mu,\\lambda,\\nu) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})} \\left(\\frac{\\lambda}{\\pi\\nu}\\right)^{\\frac{1}{2}} \\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\frac{\\nu+1}{2}}$$\n",
    "           \n",
    "where $\\Gamma$ denotes the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function), an extension of the factorial function (with its argument shifted down by 1) to real and complex numbers, and not to be confused with the (lower-case) maximum entropy [$\\gamma$ distribution](https://en.wikipedia.org/wiki/Gamma_distribution) used to model rainfalls and insurance claims. This class will, if anything, teach you the greek alphabet!\n",
    "\n",
    "The degrees-of-freedom parameter essentially specifies the \"***normality***\" of the data, since larger values of $\\nu$ make the distribution converge to a normal distribution, while small values (close to zero) result in heavier tails.\n",
    "\n",
    "Thus, the likelihood functions of our model will be specified as follows (since we seem to have outliers in our observations of IQ):\n",
    "\n",
    "$$\\begin{align}\n",
    "y^{(drug)}_i &\\sim T(\\nu, \\mu_1, \\sigma_1) \\\\\n",
    "y^{(placebo)}_i &\\sim T(\\nu, \\mu_2, \\sigma_2)\n",
    "\\end{align}$$\n",
    "\n",
    "As a simplifying assumption, we will assume that the degree of normality $\\nu$ is the same for both groups (both groups with similar outlier statistics). \n",
    "\n",
    "### Exercise\n",
    "First:\n",
    "```(python)\n",
    "pip install pymc3\n",
    "```\n",
    "\n",
    "Now, draw 10,000 samples from a Student-T distribution (`StudentT` in PyMC3) with parameter `nu=3` and compare the distribution of these values to a similar number of draws from a `Normal` distribution with parameters `mu=0` and `sd=1`. The distribution is denoted `StudentT` in `pymc3`, while the normal distribution is denoted by `Normal`. So, `StudentT.dist(nu=3)` and `Normal.dist(0,1)`. Getting a random sampling of 10,000 datapoints can be achieved with `.random(size=10000)`. Plot with `seaborn` using `.distplot()`, from -10 to 10. You should find that the Student-T is more spread out and has a lower peak than the Gaussian. Import the Student-T distribution and the Normal distribution from pymc3: `from pymc3 import StudentT, Normal`. Then, draw 10,000 random variates from Student-T: `StudentT.dist(nu=3).random(size=10000)`, and 10,000 random variates from a gaussian: `Normal.dist(0,1).random(size=10000)`. Use `seagram` to plot both histograms on top of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"divCheckbox\" style=\"display: none;\">\n",
    "from pymc3 import StudentT, Normal\n",
    "\n",
    "t = StudentT.dist(nu=6).random(size=10000)\n",
    "n = Normal.dist(0,1).random(size=10000)\n",
    "\n",
    "sns.distplot(t, label='Student-T')\n",
    "sns.distplot(n, label='Gaussian')\n",
    "plt.legend()\n",
    "plt.xlim(-10,10)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import StudentT, Normal\n",
    "\n",
    "t = StudentT.dist(nu=3).random(size=10000)\n",
    "n = Normal.dist(0,1).random(size=10000)\n",
    "\n",
    "sns.distplot(t, label='Student-T')\n",
    "sns.distplot(n, label='Gaussian')\n",
    "plt.legend()\n",
    "plt.xlim(-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing PyMC3\n",
    "-----\n",
    "\n",
    "`PyMC3` is a Python library for programming Bayesian analysis; see [here](https://doi.org/10.7717/peerj-cs.55). It's a wonderful package. Looky [here](https://docs.pymc.io/) for its API and docs. It helps us solve tough inverse problems and extract a model from the data.\n",
    "\n",
    "We will model our problem using PyMC3. This type of programming is called ***probabilistic programming***, and it is probabilistic in that we create probability models using programming variables as the model's components. Model components are first-class primitives within the PyMC3 framework. \n",
    "\n",
    ">   Another way of thinking about this: unlike a traditional program, which only runs in the forward direction, a probabilistic program runs in ***both*** forward and backward directions. It runs forward to compute consequences of assumptions it contains about the model, but also backward from the data to constrain possible explanations. In practice, many probabilistic programming systems will cleverly interleave forward and backward operations to efficiently home in on the best explanations.  - [Cronin, Beau. \"Why Probabilistic Programming Matters.\" 24 Mar 2013. Google, Online Posting to Google . Web. 24 Mar. 2013]( https://plus.google.com/u/0/107971134877020469960/posts/KpeRdJKR6Z1)\n",
    "\n",
    "PyMC3 used to rely on [**theano**](https://en.wikipedia.org/wiki/Theano_(software)), a Python library that allows one to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently, and which we will revisit when we focus on machine learning. Theano is the brainchild of [Yoshua Benjio](https://en.wikipedia.org/wiki/Yoshua_Bengio) of the University of Montreal's [MILA](https://mila.quebec/en/) laboratory. In my opinion, it's the most famous university lab associated to artificial neural networks and deep learning. It's pretty [well-funded](http://nouvelles.umontreal.ca/en/article/2017/09/15/facebook-invests-over-7m-u.s.-in-mila-and-ai-research-in-montreal/).\n",
    "\n",
    "`theano` is now deprecated because other libraries like facebook's `Torch` and Google's `TensorFlow` now include the same features. Older PyMC3 versions still uses theano, but the newer versions don't, they use [**tensorflow**](https://en.wikipedia.org/wiki/TensorFlow) instead.\n",
    "\n",
    "For probabilistic programming, you write a program in Python that builds expressions for Theano. You still have to declare variables $a,b,c$ and give their types $(int, int, int)$, build expressions for how to put those variables together $a^b + c$, and compile expression graphs to functions $Pow(a,b,c)$ in order to use them for computation. What theano builds in return is a super-fast callable object from a purely symbolic graph, optimizes the graph, and even compiles some or all of it into native machine instructions. More on Theano [here](http://www.deeplearning.net/software/theano/). \n",
    "\n",
    "For older theano versions of PyMC3, we needed to do this (don't do this if you're using a new version):\n",
    "- On Windows, from the Start menu, search for and open `Anaconda Prompt`. On MacOS, open Launchpad, then click the Terminal icon. On Linux, open a Terminal window. Now in these windows, type `conda install -c mila-udem theano pygpu`. Don't try `!conda install theano` in a jupyter notebook code cell because it may fry your jupyter notebook's kernel. Wait until success. Then in that same terminal, type `conda install pymc3`. Wait until success.\n",
    "\n",
    "PyMC3 code is easy to read. The only novel thing is the syntax. Simply remember that we are representing the model's components ($\\tau, \\lambda_1, \\lambda_2$ ) as **probabilistic variables**. And the way we represent *continuous* probabilistic variables is with a probability density function (pdf): A **function**, not a **dictionary** anymore.\n",
    "\n",
    ">**Note**: Do not confuse this/these function/s with the histogram of the dataset or the pdf that we will use to model it. That is a *different* function.\n",
    "\n",
    "To figure out these functions, we will first assume they have a certain shape, which we will call the **prior shape**.\n",
    "\n",
    "Then, we will run a simulation and try to approximate our data.\n",
    "\n",
    "This will allow us to refine the prior shapes into **posterior shapes**, in accordance with Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking our Priors\n",
    "\n",
    "We have an idea about the means of our IQ data: The data for both drug and placebo group seem to be centered around IQ = 100.\n",
    "\n",
    "So let's center the Student-T priors for $\\mu$ at 100, using a Normal distribution, and a standard deviation that is wide enough to account for plausible deviations from this population mean:\n",
    "\n",
    "$$\\mu_k \\sim N(100, 10^2)$$\n",
    "\n",
    "- *Craaaaazy*, right? I'm modeling data using a **Student-T pdf model** with 3 parameters and I model the first parameter using a **normal distribution** (pdf).\n",
    "\n",
    "Please do this below using:\n",
    "```python\n",
    "from pymc3 import Model, Uniform\n",
    "\n",
    "with Model() as drug_model:\n",
    "    μ_0 = Normal('μ_0', 100, sd=10)\n",
    "    μ_1 = Normal('μ_1', 100, sd=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use a Uniform prior for the standard deviations $\\sigma_k$ of the Student-T, with a lower bound of 0 and an upper bound of 20, here below:\n",
    "```(python)\n",
    "with drug_model:\n",
    "    σ_0 = Uniform('σ_0', lower=0, upper=20)\n",
    "    σ_1 = Uniform('σ_1', lower=0, upper=20)\n",
    "```\n",
    "\n",
    "- *Craaaazy*! I use a **uniform distribution** (pdf) to model the second parameter of the pdf-based model (standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the degrees-of-freedom parameter $\\nu$, use an exponential distribution with a mean of 30. \n",
    "```python\n",
    "from pymc3 import Exponential\n",
    "sns.distplot(Exponential.dist(1/29).random(size=10000), kde=False);\n",
    "```\n",
    "\n",
    "- *Craaazy*! I use an **exponential distribution** to model the third parameter of my **student-T model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allocates ***high prior probability*** over the regions of the parameter that describe the range from normal to heavy-tailed data under the Student-T distribution:\n",
    "```python\n",
    "with drug_model:\n",
    "    ν = Exponential('ν_minus_one', 1/29.) + 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go ahead and model both datasets in `pymc3`:\n",
    "```python\n",
    "with drug_model:\n",
    "    drug_like = StudentT('drug_like', nu=ν, mu=μ_1, lam=σ_1**2, observed=drug.iq)\n",
    "    placebo_like = StudentT('placebo_like', nu=ν, mu=μ_0, lam=σ_0**2, observed=placebo.iq)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn your attention now to tracking the **posterior** ***quantities of interest***. Namely, calculate the difference in means between the drug and placebo groups: `diff_of_means = Deterministic('difference of means', μ_1 - μ_0)`. \n",
    "\n",
    "As a joint measure of the groups, also estimate the [**effect size**](https://en.wikipedia.org/wiki/Effect_size), which is the difference in means scaled by the pooled (square root of the squares) estimates of standard deviation: `Deterministic('effect size', diff_of_means / np.sqrt((σ_1**2 + σ_0**2) / 2))`. \n",
    "\n",
    "The effect size resembles the computation for a t-test statistic, with the critical difference that the t-test statistic includes a factor of ${\\sqrt {n}}$. This means that for a given effect size, the significance level increases with the sample size. Unlike the t-test statistic, the effect size aims to estimate a population parameter and is not affected by the sample size (yay!). Effect size can be harder to interpret, since it is no longer in the same units as our data, but it is a function of all four estimated parameters.\n",
    "\n",
    "We need to specify `Deterministic` because all PyMC3 variables are by default **probabilistic**, unless we set them to be **deterministic**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Deterministic\n",
    "    \n",
    "with drug_model:\n",
    "    diff_of_means = Deterministic('difference of means', μ_1 - μ_0)\n",
    "    effect_size = Deterministic('effect size', diff_of_means / np.sqrt((σ_1**2 + σ_0**2) / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, all right, all right! Now our model is **fully specified** and we are ready to track our posteriors.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://media1.tenor.com/images/c4b036354e1a6e6fedd3809a0c945003/tenor.gif?itemid=5146096\" width=\"400\" />\n",
    "All right, all right, all right\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit the model using [**variational inference**](https://en.wikipedia.org/wiki/Variational_Bayesian_methods). This will estimate all our posterior distributions using an optimized approximation, and then draw 1,000 samples from it. Be *patient* now, we are running **probabilistic regressions**.\n",
    "```python\n",
    "from pymc3 import fit\n",
    "\n",
    "with drug_model: \n",
    "    drug_trace = fit(random_seed=RANDOM_SEED).sample(1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 263.99: 100%|██████████| 10000/10000 [00:05<00:00, 1987.60it/s]\n",
      "Finished [100%]: Average Loss = 263.72\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Waaaaait*** for the probabilistic computation to finish!\n",
    "\n",
    ">**Note**: You *may* have to speify the extra argument (inside the `fit()` API): `cores = 1` if your laptop does not have the power to run all 4 cores (the *old* API analog was `njobs = 1`)\n",
    "\n",
    "Now plot all your posterior distributions, throwing away the first 100 samples. You typically always throw away from 10% to 20% of your simulation samples, because they start off *wrong* before converging to the *right solution*:\n",
    "```python\n",
    "from pymc3 import plot_posterior\n",
    "\n",
    "plot_posterior(drug_trace[100:], \n",
    "                varnames=['μ_0', 'μ_1', 'σ_0', 'σ_1', 'ν_minus_one'],\n",
    "                color='#87ceeb');\n",
    "```\n",
    "\n",
    "and in the next cell:\n",
    "```python\n",
    "plot_posterior(drug_trace[100:], \n",
    "          varnames=['difference of means', 'effect size'],\n",
    "          ref_val=0,\n",
    "          color='#87ceeb');\n",
    "```\n",
    "\n",
    "if you have import errors on `plot_posterior`, chances are the APIs got moved to another library. So do this instead:\n",
    "```(python)\n",
    "pip install arviz\n",
    "from arviz import plot_posterior\n",
    "```\n",
    "\n",
    "Also, possible that `varnames` was renamed to `var_names`.\n",
    "\n",
    "In any case, any error, please google the error. If not solution, let us know :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, *conclude* please.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "- Conclusion: The posterior probability that the mean IQ of the subjects in the treatment group is greater than that of the placebo group is left of zero. That means that all the probability that the drug *worked* is concentrated beyond the null hypothesis (0), the effect of the drug is around 30%, and the most probable value in the difference between the drug group and the control group is a difference of 1 in the first parameter of the model assumed. So the IQ drug *worked*!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclusion: The posterior probability ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "- Find a single-humped dataset on the Web. The most original datasets will get a bonus. \n",
    "- Compute the histogram, and make it look pretty.\n",
    "- Figure out the best possible match using MLE to find the best parameters.\n",
    "- Now, repeat the experiment with Bayesian simulation using PyMC3.\n",
    "- Which method is more correct, and which was the most fun?\n",
    "\n",
    "## References and Resources\n",
    "\n",
    "- Goodman, S. N. (1999). Toward evidence-based medical statistics. 1: The P value fallacy. Annals of Internal Medicine, 130(12), 995–1004. http://doi.org/10.7326/0003-4819-130-12-199906150-00008\n",
    "- Johnson, D. (1999). The insignificance of statistical significance testing. Journal of Wildlife Management, 63(3), 763–772.\n",
    "- Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis, Third Edition. CRC Press.\n",
    "-  Norvig, Peter. 2009. [The Unreasonable Effectiveness of Data](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf).\n",
    "- Salvatier, J, Wiecki TV, and Fonnesbeck C. (2016) Probabilistic programming in Python using PyMC3. *PeerJ Computer Science* 2:e55 <https://doi.org/10.7717/peerj-cs.55>\n",
    "- Cronin, Beau. \"Why Probabilistic Programming Matters.\" 24 Mar 2013. Google, Online Posting to Google . Web. 24 Mar. 2013. <https://plus.google.com/u/0/107971134877020469960/posts/KpeRdJKR6Z1>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
